{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-dotenv in ./venv/lib/python3.13/site-packages (1.1.1)\n",
            "Requirement already satisfied: pandas in ./venv/lib/python3.13/site-packages (2.3.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in ./venv/lib/python3.13/site-packages (from pandas) (2.3.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install python-dotenv pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DnrjRcVxX6SC"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import json\n",
        "import yaml\n",
        "\n",
        "load_dotenv() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sMH0b1aSZttG"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "GIT_REPO_FILTERED = \"files/git_repo_filtered_js_commit_date.csv\"\n",
        "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
        "GRAPHQL_URL = \"https://api.github.com/graphql\"\n",
        "HEADERS = {\n",
        "    \"Authorization\": f\"Bearer {GITHUB_TOKEN}\",\n",
        "    \"Accept\": \"application/vnd.github+json\"\n",
        "}\n",
        "\n",
        "repos_from_csv = []\n",
        "\n",
        "repos_with_CI = set()\n",
        "invalid_repos = set()\n",
        "repos_with_no_workflows = set()\n",
        "repos_with_network_error = set()\n",
        "repo_name_to_branch = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgFPoYf2Z89i",
        "outputId": "bd4ba4ea-056c-4028-87a3-d58613397d7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'owner': 'bigbluebutton', 'name': 'bigbluebutton', 'default_branch': 'v3.0.x-release'}, {'owner': 'zabinx', 'name': 'duskrpg', 'default_branch': 'master'}, {'owner': 'apache', 'name': 'cordova-android', 'default_branch': 'master'}, {'owner': 'aws-samples', 'name': 'aws-dynamodb-examples', 'default_branch': 'master'}, {'owner': 'dgarijo', 'name': 'widoco', 'default_branch': 'master'}]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def read_filtered_repo_csvlist():\n",
        "    df = pd.read_csv(GIT_REPO_FILTERED)\n",
        "\n",
        "    # Split 'name' into owner and repo\n",
        "    for _, row in df.iterrows():\n",
        "        if '/' not in row['name']:\n",
        "            print(f\"Skipping invalid repo name: {row['name']}\")\n",
        "            continue\n",
        "        owner, repo_name = row['name'].split('/', 1)\n",
        "        repos_from_csv.append({\n",
        "            \"owner\": owner.strip(),\n",
        "            \"name\": repo_name.strip(),\n",
        "            \"default_branch\": row['default_branch'].strip()\n",
        "        })\n",
        "        repo_name_to_branch[f\"{owner.strip()}/{repo_name.strip()}\".lower()] = row['default_branch'].strip()\n",
        "\n",
        "    print(repos_from_csv[:5])\n",
        "\n",
        "read_filtered_repo_csvlist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "R2Ra1Cy3aJKd"
      },
      "outputs": [],
      "source": [
        "# Step 2: Build GraphQL query for multiple repos\n",
        "def build_query_to_check_workflows(repos, start, end):\n",
        "    query_parts = []\n",
        "    for i, repo in enumerate(repos[start:end]):\n",
        "        query_parts.append(f\"\"\"\n",
        "        repo{i}: repository(owner: \"{repo['owner']}\", name: \"{repo['name']}\") {{\n",
        "            workflows: object(expression: \"{repo['default_branch']}:.github/workflows\") {{\n",
        "                ... on Tree {{\n",
        "                    entries {{\n",
        "                        name\n",
        "                        type\n",
        "                    }}\n",
        "                }}\n",
        "            }}\n",
        "        }}\n",
        "        \"\"\")\n",
        "    full_query = \"query { \" + \" \".join(query_parts) + \" }\"\n",
        "    # print(\"query:\", full_query)\n",
        "    return full_query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6Me8Pr-Kdm2T"
      },
      "outputs": [],
      "source": [
        "# Step 3: Execute query and parse results\n",
        "def check_workflows(repos, start, end):\n",
        "    try:\n",
        "        query = build_query_to_check_workflows(repos, start, end)\n",
        "        response = requests.post(GRAPHQL_URL, json={\"query\": query}, headers=HEADERS)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        # If GitHub responds with an error (403, 502, etc.)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Skipping batch {start}:{end} (HTTP {response.status_code})\")\n",
        "            for repo in repos[start:end]:\n",
        "                repos_with_network_error.add(f\"{repo['owner']}/{repo['name']}\")\n",
        "            return\n",
        "\n",
        "        data = response.json()\n",
        "\n",
        "        for i, repo in enumerate(repos[start:end]):\n",
        "            key = f\"repo{i}\"\n",
        "            repo_data = data.get(\"data\", {}).get(key, {})\n",
        "\n",
        "            if not repo_data:\n",
        "                invalid_repos.add(f\"{repo['owner']}/{repo['name']}\")\n",
        "                continue\n",
        "            workflows = repo_data.get(\"workflows\")\n",
        "\n",
        "            if workflows and workflows.get(\"entries\"):\n",
        "                repos_with_CI.add(f\"{repo['owner']}/{repo['name']}\")\n",
        "            else:\n",
        "                repos_with_no_workflows.add(f\"{repo['owner']}/{repo['name']}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Request failed for batch {start}:{end}: {e}\")\n",
        "        for repo in repos[start:end]:\n",
        "            repos_with_network_error.add(f\"{repo['owner']}/{repo['name']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omUMFpZUdotq",
        "outputId": "6055f9cd-0959-475d-9819-857737900fa6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 0 to 100. repos_with_CI:54 repos_with_network_error:0 repos_with_no_workflows:45 invalid_repos:1\n",
            "Processed 100 to 200. repos_with_CI:120 repos_with_network_error:0 repos_with_no_workflows:77 invalid_repos:3\n",
            "Processed 200 to 300. repos_with_CI:152 repos_with_network_error:0 repos_with_no_workflows:144 invalid_repos:4\n",
            "Processed 300 to 400. repos_with_CI:177 repos_with_network_error:0 repos_with_no_workflows:219 invalid_repos:4\n",
            "Processed 400 to 500. repos_with_CI:194 repos_with_network_error:0 repos_with_no_workflows:301 invalid_repos:5\n",
            "Processed 500 to 600. repos_with_CI:228 repos_with_network_error:0 repos_with_no_workflows:366 invalid_repos:6\n",
            "Processed 600 to 700. repos_with_CI:259 repos_with_network_error:0 repos_with_no_workflows:435 invalid_repos:6\n",
            "Processed 700 to 800. repos_with_CI:283 repos_with_network_error:0 repos_with_no_workflows:511 invalid_repos:6\n",
            "Processed 800 to 900. repos_with_CI:310 repos_with_network_error:0 repos_with_no_workflows:583 invalid_repos:7\n",
            "Processed 900 to 1000. repos_with_CI:338 repos_with_network_error:0 repos_with_no_workflows:655 invalid_repos:7\n",
            "Processed 1000 to 1100. repos_with_CI:373 repos_with_network_error:0 repos_with_no_workflows:720 invalid_repos:7\n",
            "Processed 1100 to 1200. repos_with_CI:412 repos_with_network_error:0 repos_with_no_workflows:781 invalid_repos:7\n",
            "Processed 1200 to 1300. repos_with_CI:457 repos_with_network_error:0 repos_with_no_workflows:836 invalid_repos:7\n",
            "Processed 1300 to 1400. repos_with_CI:490 repos_with_network_error:0 repos_with_no_workflows:902 invalid_repos:8\n",
            "Processed 1400 to 1500. repos_with_CI:537 repos_with_network_error:0 repos_with_no_workflows:955 invalid_repos:8\n",
            "Processed 1500 to 1600. repos_with_CI:582 repos_with_network_error:0 repos_with_no_workflows:1009 invalid_repos:9\n",
            "Processed 1600 to 1700. repos_with_CI:633 repos_with_network_error:0 repos_with_no_workflows:1058 invalid_repos:9\n",
            "Processed 1700 to 1800. repos_with_CI:686 repos_with_network_error:0 repos_with_no_workflows:1105 invalid_repos:9\n",
            "Processed 1800 to 1900. repos_with_CI:733 repos_with_network_error:0 repos_with_no_workflows:1158 invalid_repos:9\n",
            "Processed 1900 to 2000. repos_with_CI:777 repos_with_network_error:0 repos_with_no_workflows:1214 invalid_repos:9\n",
            "Processed 2000 to 2100. repos_with_CI:824 repos_with_network_error:0 repos_with_no_workflows:1266 invalid_repos:10\n",
            "Processed 2100 to 2200. repos_with_CI:869 repos_with_network_error:0 repos_with_no_workflows:1321 invalid_repos:10\n",
            "Processed 2200 to 2300. repos_with_CI:913 repos_with_network_error:0 repos_with_no_workflows:1376 invalid_repos:11\n",
            "Processed 2300 to 2400. repos_with_CI:965 repos_with_network_error:0 repos_with_no_workflows:1424 invalid_repos:11\n",
            "Processed 2400 to 2500. repos_with_CI:1014 repos_with_network_error:0 repos_with_no_workflows:1475 invalid_repos:11\n",
            "Processed 2500 to 2600. repos_with_CI:1059 repos_with_network_error:0 repos_with_no_workflows:1530 invalid_repos:11\n",
            "Processed 2600 to 2700. repos_with_CI:1116 repos_with_network_error:0 repos_with_no_workflows:1571 invalid_repos:13\n",
            "Processed 2700 to 2800. repos_with_CI:1156 repos_with_network_error:0 repos_with_no_workflows:1631 invalid_repos:13\n",
            "Processed 2800 to 2900. repos_with_CI:1205 repos_with_network_error:0 repos_with_no_workflows:1682 invalid_repos:13\n",
            "Processed 2900 to 3000. repos_with_CI:1256 repos_with_network_error:0 repos_with_no_workflows:1731 invalid_repos:13\n",
            "Processed 3000 to 3100. repos_with_CI:1310 repos_with_network_error:0 repos_with_no_workflows:1776 invalid_repos:14\n",
            "Processed 3100 to 3200. repos_with_CI:1366 repos_with_network_error:0 repos_with_no_workflows:1820 invalid_repos:14\n",
            "Processed 3200 to 3300. repos_with_CI:1413 repos_with_network_error:0 repos_with_no_workflows:1873 invalid_repos:14\n",
            "Processed 3300 to 3400. repos_with_CI:1466 repos_with_network_error:0 repos_with_no_workflows:1920 invalid_repos:14\n",
            "Processed 3400 to 3500. repos_with_CI:1513 repos_with_network_error:0 repos_with_no_workflows:1973 invalid_repos:14\n",
            "Processed 3500 to 3600. repos_with_CI:1565 repos_with_network_error:0 repos_with_no_workflows:2019 invalid_repos:16\n",
            "Processed 3600 to 3700. repos_with_CI:1618 repos_with_network_error:0 repos_with_no_workflows:2066 invalid_repos:16\n",
            "Processed 3700 to 3800. repos_with_CI:1674 repos_with_network_error:0 repos_with_no_workflows:2110 invalid_repos:16\n",
            "Processed 3800 to 3900. repos_with_CI:1730 repos_with_network_error:0 repos_with_no_workflows:2153 invalid_repos:17\n",
            "Processed 3900 to 4000. repos_with_CI:1786 repos_with_network_error:0 repos_with_no_workflows:2197 invalid_repos:17\n",
            "Processed 4000 to 4100. repos_with_CI:1836 repos_with_network_error:0 repos_with_no_workflows:2244 invalid_repos:20\n",
            "Processed 4100 to 4200. repos_with_CI:1886 repos_with_network_error:0 repos_with_no_workflows:2294 invalid_repos:20\n",
            "Processed 4200 to 4300. repos_with_CI:1947 repos_with_network_error:0 repos_with_no_workflows:2333 invalid_repos:20\n",
            "Processed 4300 to 4400. repos_with_CI:1996 repos_with_network_error:0 repos_with_no_workflows:2383 invalid_repos:21\n",
            "Processed 4400 to 4500. repos_with_CI:2053 repos_with_network_error:0 repos_with_no_workflows:2426 invalid_repos:21\n",
            "Processed 4500 to 4600. repos_with_CI:2116 repos_with_network_error:0 repos_with_no_workflows:2462 invalid_repos:22\n",
            "Processed 4600 to 4700. repos_with_CI:2176 repos_with_network_error:0 repos_with_no_workflows:2499 invalid_repos:25\n",
            "Processed 4700 to 4800. repos_with_CI:2229 repos_with_network_error:0 repos_with_no_workflows:2545 invalid_repos:26\n",
            "Processed 4800 to 4900. repos_with_CI:2286 repos_with_network_error:0 repos_with_no_workflows:2588 invalid_repos:26\n",
            "Processed 4900 to 5000. repos_with_CI:2346 repos_with_network_error:0 repos_with_no_workflows:2628 invalid_repos:26\n",
            "Processed 5000 to 5100. repos_with_CI:2391 repos_with_network_error:0 repos_with_no_workflows:2683 invalid_repos:26\n",
            "Processed 5100 to 5200. repos_with_CI:2447 repos_with_network_error:0 repos_with_no_workflows:2727 invalid_repos:26\n",
            "Processed 5200 to 5300. repos_with_CI:2516 repos_with_network_error:0 repos_with_no_workflows:2757 invalid_repos:27\n",
            "Processed 5300 to 5400. repos_with_CI:2577 repos_with_network_error:0 repos_with_no_workflows:2796 invalid_repos:27\n",
            "Processed 5400 to 5500. repos_with_CI:2651 repos_with_network_error:0 repos_with_no_workflows:2822 invalid_repos:27\n",
            "Processed 5500 to 5600. repos_with_CI:2706 repos_with_network_error:0 repos_with_no_workflows:2867 invalid_repos:27\n",
            "Processed 5600 to 5700. repos_with_CI:2769 repos_with_network_error:0 repos_with_no_workflows:2903 invalid_repos:28\n",
            "Processed 5700 to 5800. repos_with_CI:2830 repos_with_network_error:0 repos_with_no_workflows:2942 invalid_repos:28\n",
            "Processed 5800 to 5900. repos_with_CI:2887 repos_with_network_error:0 repos_with_no_workflows:2985 invalid_repos:28\n",
            "Processed 5900 to 6000. repos_with_CI:2940 repos_with_network_error:0 repos_with_no_workflows:3030 invalid_repos:30\n",
            "Processed 6000 to 6100. repos_with_CI:2995 repos_with_network_error:0 repos_with_no_workflows:3075 invalid_repos:30\n",
            "Processed 6100 to 6200. repos_with_CI:3055 repos_with_network_error:0 repos_with_no_workflows:3114 invalid_repos:31\n",
            "Processed 6200 to 6300. repos_with_CI:3116 repos_with_network_error:0 repos_with_no_workflows:3153 invalid_repos:31\n",
            "Processed 6300 to 6400. repos_with_CI:3186 repos_with_network_error:0 repos_with_no_workflows:3183 invalid_repos:31\n",
            "Processed 6400 to 6500. repos_with_CI:3247 repos_with_network_error:0 repos_with_no_workflows:3220 invalid_repos:33\n",
            "Processed 6500 to 6600. repos_with_CI:3311 repos_with_network_error:0 repos_with_no_workflows:3256 invalid_repos:33\n",
            "Processed 6600 to 6700. repos_with_CI:3379 repos_with_network_error:0 repos_with_no_workflows:3288 invalid_repos:33\n",
            "Processed 6700 to 6800. repos_with_CI:3449 repos_with_network_error:0 repos_with_no_workflows:3317 invalid_repos:34\n",
            "Processed 6800 to 6900. repos_with_CI:3501 repos_with_network_error:0 repos_with_no_workflows:3365 invalid_repos:34\n",
            "Processed 6900 to 7000. repos_with_CI:3569 repos_with_network_error:0 repos_with_no_workflows:3397 invalid_repos:34\n",
            "Processed 7000 to 7100. repos_with_CI:3631 repos_with_network_error:0 repos_with_no_workflows:3434 invalid_repos:35\n",
            "Processed 7100 to 7200. repos_with_CI:3699 repos_with_network_error:0 repos_with_no_workflows:3466 invalid_repos:35\n",
            "Processed 7200 to 7300. repos_with_CI:3762 repos_with_network_error:0 repos_with_no_workflows:3503 invalid_repos:35\n",
            "Processed 7300 to 7400. repos_with_CI:3829 repos_with_network_error:0 repos_with_no_workflows:3536 invalid_repos:35\n",
            "Processed 7400 to 7500. repos_with_CI:3894 repos_with_network_error:0 repos_with_no_workflows:3571 invalid_repos:35\n",
            "Processed 7500 to 7600. repos_with_CI:3973 repos_with_network_error:0 repos_with_no_workflows:3592 invalid_repos:35\n",
            "Processed 7600 to 7700. repos_with_CI:4043 repos_with_network_error:0 repos_with_no_workflows:3620 invalid_repos:37\n",
            "Processed 7700 to 7800. repos_with_CI:4114 repos_with_network_error:0 repos_with_no_workflows:3649 invalid_repos:37\n",
            "Processed 7800 to 7900. repos_with_CI:4174 repos_with_network_error:0 repos_with_no_workflows:3689 invalid_repos:37\n",
            "Processed 7900 to 8000. repos_with_CI:4248 repos_with_network_error:0 repos_with_no_workflows:3715 invalid_repos:37\n",
            "Processed 8000 to 8100. repos_with_CI:4311 repos_with_network_error:0 repos_with_no_workflows:3751 invalid_repos:38\n",
            "Processed 8100 to 8200. repos_with_CI:4386 repos_with_network_error:0 repos_with_no_workflows:3776 invalid_repos:38\n",
            "Processed 8200 to 8300. repos_with_CI:4450 repos_with_network_error:0 repos_with_no_workflows:3812 invalid_repos:38\n",
            "Processed 8300 to 8400. repos_with_CI:4521 repos_with_network_error:0 repos_with_no_workflows:3841 invalid_repos:38\n",
            "Processed 8400 to 8500. repos_with_CI:4597 repos_with_network_error:0 repos_with_no_workflows:3865 invalid_repos:38\n",
            "Processed 8500 to 8600. repos_with_CI:4681 repos_with_network_error:0 repos_with_no_workflows:3881 invalid_repos:38\n",
            "Request failed for batch 8600:8700: 502 Server Error: Bad Gateway for url: https://api.github.com/graphql\n",
            "Processed 8600 to 8700. repos_with_CI:4681 repos_with_network_error:100 repos_with_no_workflows:3881 invalid_repos:38\n",
            "Processed 8700 to 8800. repos_with_CI:4749 repos_with_network_error:100 repos_with_no_workflows:3912 invalid_repos:39\n",
            "Processed 8800 to 8900. repos_with_CI:4824 repos_with_network_error:100 repos_with_no_workflows:3936 invalid_repos:40\n",
            "Processed 8900 to 9000. repos_with_CI:4905 repos_with_network_error:100 repos_with_no_workflows:3952 invalid_repos:43\n",
            "Processed 9000 to 9100. repos_with_CI:4966 repos_with_network_error:100 repos_with_no_workflows:3991 invalid_repos:43\n",
            "Processed 9100 to 9200. repos_with_CI:5030 repos_with_network_error:100 repos_with_no_workflows:4027 invalid_repos:43\n",
            "Processed 9200 to 9300. repos_with_CI:5087 repos_with_network_error:100 repos_with_no_workflows:4070 invalid_repos:43\n",
            "Processed 9300 to 9400. repos_with_CI:5140 repos_with_network_error:100 repos_with_no_workflows:4117 invalid_repos:43\n",
            "Processed 9400 to 9500. repos_with_CI:5198 repos_with_network_error:100 repos_with_no_workflows:4159 invalid_repos:43\n",
            "Processed 9500 to 9600. repos_with_CI:5261 repos_with_network_error:100 repos_with_no_workflows:4196 invalid_repos:43\n",
            "Processed 9600 to 9700. repos_with_CI:5316 repos_with_network_error:100 repos_with_no_workflows:4240 invalid_repos:44\n",
            "Processed 9700 to 9800. repos_with_CI:5371 repos_with_network_error:100 repos_with_no_workflows:4284 invalid_repos:45\n",
            "Processed 9800 to 9900. repos_with_CI:5436 repos_with_network_error:100 repos_with_no_workflows:4319 invalid_repos:45\n",
            "Processed 9900 to 10000. repos_with_CI:5499 repos_with_network_error:100 repos_with_no_workflows:4356 invalid_repos:45\n",
            "Processed 10000 to 10100. repos_with_CI:5557 repos_with_network_error:100 repos_with_no_workflows:4396 invalid_repos:47\n",
            "Processed 10100 to 10200. repos_with_CI:5615 repos_with_network_error:100 repos_with_no_workflows:4438 invalid_repos:47\n",
            "Processed 10200 to 10300. repos_with_CI:5672 repos_with_network_error:100 repos_with_no_workflows:4479 invalid_repos:49\n",
            "Processed 10300 to 10400. repos_with_CI:5731 repos_with_network_error:100 repos_with_no_workflows:4518 invalid_repos:51\n",
            "Processed 10400 to 10500. repos_with_CI:5788 repos_with_network_error:100 repos_with_no_workflows:4560 invalid_repos:52\n",
            "Processed 10500 to 10600. repos_with_CI:5847 repos_with_network_error:100 repos_with_no_workflows:4600 invalid_repos:53\n",
            "Processed 10600 to 10700. repos_with_CI:5903 repos_with_network_error:100 repos_with_no_workflows:4644 invalid_repos:53\n",
            "Processed 10700 to 10800. repos_with_CI:5948 repos_with_network_error:100 repos_with_no_workflows:4698 invalid_repos:54\n",
            "Processed 10800 to 10900. repos_with_CI:5998 repos_with_network_error:100 repos_with_no_workflows:4747 invalid_repos:55\n",
            "Processed 10900 to 11000. repos_with_CI:6046 repos_with_network_error:100 repos_with_no_workflows:4799 invalid_repos:55\n",
            "Processed 11000 to 11100. repos_with_CI:6093 repos_with_network_error:100 repos_with_no_workflows:4851 invalid_repos:56\n",
            "Processed 11100 to 11200. repos_with_CI:6146 repos_with_network_error:100 repos_with_no_workflows:4898 invalid_repos:56\n",
            "Processed 11200 to 11300. repos_with_CI:6201 repos_with_network_error:100 repos_with_no_workflows:4942 invalid_repos:57\n",
            "Processed 11300 to 11400. repos_with_CI:6249 repos_with_network_error:100 repos_with_no_workflows:4994 invalid_repos:57\n",
            "Processed 11400 to 11500. repos_with_CI:6297 repos_with_network_error:100 repos_with_no_workflows:5046 invalid_repos:57\n",
            "Processed 11500 to 11600. repos_with_CI:6346 repos_with_network_error:100 repos_with_no_workflows:5096 invalid_repos:58\n",
            "Processed 11600 to 11700. repos_with_CI:6403 repos_with_network_error:100 repos_with_no_workflows:5137 invalid_repos:60\n",
            "Processed 11700 to 11800. repos_with_CI:6453 repos_with_network_error:100 repos_with_no_workflows:5186 invalid_repos:61\n",
            "Processed 11800 to 11900. repos_with_CI:6512 repos_with_network_error:100 repos_with_no_workflows:5226 invalid_repos:62\n",
            "Processed 11900 to 12000. repos_with_CI:6570 repos_with_network_error:100 repos_with_no_workflows:5266 invalid_repos:64\n",
            "Processed 12000 to 12100. repos_with_CI:6620 repos_with_network_error:100 repos_with_no_workflows:5314 invalid_repos:66\n",
            "Processed 12100 to 12200. repos_with_CI:6675 repos_with_network_error:100 repos_with_no_workflows:5356 invalid_repos:69\n",
            "Processed 12200 to 12300. repos_with_CI:6732 repos_with_network_error:100 repos_with_no_workflows:5397 invalid_repos:71\n",
            "Processed 12300 to 12400. repos_with_CI:6782 repos_with_network_error:100 repos_with_no_workflows:5447 invalid_repos:71\n",
            "Processed 12400 to 12500. repos_with_CI:6839 repos_with_network_error:100 repos_with_no_workflows:5490 invalid_repos:71\n",
            "Processed 12500 to 12600. repos_with_CI:6892 repos_with_network_error:100 repos_with_no_workflows:5535 invalid_repos:73\n",
            "Processed 12600 to 12700. repos_with_CI:6951 repos_with_network_error:100 repos_with_no_workflows:5576 invalid_repos:73\n",
            "Processed 12700 to 12800. repos_with_CI:7002 repos_with_network_error:100 repos_with_no_workflows:5622 invalid_repos:76\n",
            "Processed 12800 to 12900. repos_with_CI:7054 repos_with_network_error:100 repos_with_no_workflows:5668 invalid_repos:78\n",
            "Processed 12900 to 13000. repos_with_CI:7106 repos_with_network_error:100 repos_with_no_workflows:5713 invalid_repos:81\n",
            "Processed 13000 to 13100. repos_with_CI:7147 repos_with_network_error:100 repos_with_no_workflows:5772 invalid_repos:81\n",
            "Processed 13100 to 13200. repos_with_CI:7205 repos_with_network_error:100 repos_with_no_workflows:5814 invalid_repos:81\n",
            "Processed 13200 to 13300. repos_with_CI:7259 repos_with_network_error:100 repos_with_no_workflows:5859 invalid_repos:82\n",
            "Processed 13300 to 13400. repos_with_CI:7307 repos_with_network_error:100 repos_with_no_workflows:5911 invalid_repos:82\n",
            "Processed 13400 to 13500. repos_with_CI:7349 repos_with_network_error:100 repos_with_no_workflows:5969 invalid_repos:82\n",
            "Processed 13500 to 13600. repos_with_CI:7406 repos_with_network_error:100 repos_with_no_workflows:6011 invalid_repos:83\n",
            "Processed 13600 to 13700. repos_with_CI:7456 repos_with_network_error:100 repos_with_no_workflows:6059 invalid_repos:85\n",
            "Processed 13700 to 13800. repos_with_CI:7513 repos_with_network_error:100 repos_with_no_workflows:6101 invalid_repos:86\n",
            "Processed 13800 to 13900. repos_with_CI:7555 repos_with_network_error:100 repos_with_no_workflows:6159 invalid_repos:86\n",
            "Processed 13900 to 14000. repos_with_CI:7612 repos_with_network_error:100 repos_with_no_workflows:6201 invalid_repos:87\n",
            "Processed 14000 to 14100. repos_with_CI:7660 repos_with_network_error:100 repos_with_no_workflows:6252 invalid_repos:88\n",
            "Processed 14100 to 14200. repos_with_CI:7706 repos_with_network_error:100 repos_with_no_workflows:6304 invalid_repos:90\n",
            "Processed 14200 to 14300. repos_with_CI:7768 repos_with_network_error:100 repos_with_no_workflows:6341 invalid_repos:91\n",
            "Processed 14300 to 14400. repos_with_CI:7811 repos_with_network_error:100 repos_with_no_workflows:6397 invalid_repos:92\n",
            "Processed 14400 to 14500. repos_with_CI:7854 repos_with_network_error:100 repos_with_no_workflows:6454 invalid_repos:92\n",
            "Processed 14500 to 14600. repos_with_CI:7910 repos_with_network_error:100 repos_with_no_workflows:6495 invalid_repos:95\n",
            "Processed 14600 to 14700. repos_with_CI:7954 repos_with_network_error:100 repos_with_no_workflows:6547 invalid_repos:99\n",
            "Processed 14700 to 14800. repos_with_CI:8009 repos_with_network_error:100 repos_with_no_workflows:6592 invalid_repos:99\n",
            "Processed 14800 to 14900. repos_with_CI:8055 repos_with_network_error:100 repos_with_no_workflows:6645 invalid_repos:100\n",
            "Processed 14900 to 15000. repos_with_CI:8106 repos_with_network_error:100 repos_with_no_workflows:6692 invalid_repos:102\n",
            "Processed 15000 to 15100. repos_with_CI:8153 repos_with_network_error:100 repos_with_no_workflows:6744 invalid_repos:103\n",
            "Processed 15100 to 15200. repos_with_CI:8208 repos_with_network_error:100 repos_with_no_workflows:6787 invalid_repos:105\n",
            "Processed 15200 to 15300. repos_with_CI:8262 repos_with_network_error:100 repos_with_no_workflows:6833 invalid_repos:105\n",
            "Processed 15300 to 15400. repos_with_CI:8316 repos_with_network_error:100 repos_with_no_workflows:6879 invalid_repos:105\n",
            "Processed 15400 to 15500. repos_with_CI:8357 repos_with_network_error:100 repos_with_no_workflows:6938 invalid_repos:105\n",
            "Processed 15500 to 15600. repos_with_CI:8406 repos_with_network_error:100 repos_with_no_workflows:6985 invalid_repos:109\n",
            "Processed 15600 to 15700. repos_with_CI:8462 repos_with_network_error:100 repos_with_no_workflows:7028 invalid_repos:110\n",
            "Processed 15700 to 15800. repos_with_CI:8521 repos_with_network_error:100 repos_with_no_workflows:7067 invalid_repos:112\n",
            "Processed 15800 to 15900. repos_with_CI:8575 repos_with_network_error:100 repos_with_no_workflows:7112 invalid_repos:113\n",
            "Processed 15900 to 16000. repos_with_CI:8629 repos_with_network_error:100 repos_with_no_workflows:7156 invalid_repos:115\n",
            "Processed 16000 to 16100. repos_with_CI:8679 repos_with_network_error:100 repos_with_no_workflows:7205 invalid_repos:116\n",
            "Processed 16100 to 16200. repos_with_CI:8734 repos_with_network_error:100 repos_with_no_workflows:7249 invalid_repos:117\n",
            "Processed 16200 to 16300. repos_with_CI:8782 repos_with_network_error:100 repos_with_no_workflows:7299 invalid_repos:119\n",
            "Processed 16300 to 16400. repos_with_CI:8821 repos_with_network_error:100 repos_with_no_workflows:7360 invalid_repos:119\n",
            "Processed 16400 to 16500. repos_with_CI:8864 repos_with_network_error:100 repos_with_no_workflows:7415 invalid_repos:121\n",
            "Processed 16500 to 16600. repos_with_CI:8912 repos_with_network_error:100 repos_with_no_workflows:7466 invalid_repos:122\n",
            "Processed 16600 to 16700. repos_with_CI:8955 repos_with_network_error:100 repos_with_no_workflows:7520 invalid_repos:125\n",
            "Request failed for batch 16700:16800: 502 Server Error: Bad Gateway for url: https://api.github.com/graphql\n",
            "Processed 16700 to 16800. repos_with_CI:8955 repos_with_network_error:200 repos_with_no_workflows:7520 invalid_repos:125\n",
            "Processed 16800 to 16900. repos_with_CI:9020 repos_with_network_error:200 repos_with_no_workflows:7553 invalid_repos:127\n",
            "Processed 16900 to 17000. repos_with_CI:9081 repos_with_network_error:200 repos_with_no_workflows:7591 invalid_repos:128\n",
            "Processed 17000 to 17100. repos_with_CI:9129 repos_with_network_error:200 repos_with_no_workflows:7641 invalid_repos:130\n",
            "Processed 17100 to 17200. repos_with_CI:9189 repos_with_network_error:200 repos_with_no_workflows:7679 invalid_repos:132\n",
            "Processed 17200 to 17300. repos_with_CI:9245 repos_with_network_error:200 repos_with_no_workflows:7717 invalid_repos:138\n",
            "Processed 17300 to 17400. repos_with_CI:9293 repos_with_network_error:200 repos_with_no_workflows:7767 invalid_repos:140\n",
            "Processed 17400 to 17500. repos_with_CI:9349 repos_with_network_error:200 repos_with_no_workflows:7810 invalid_repos:141\n",
            "Processed 17500 to 17600. repos_with_CI:9380 repos_with_network_error:200 repos_with_no_workflows:7877 invalid_repos:143\n",
            "Processed 17600 to 17700. repos_with_CI:9423 repos_with_network_error:200 repos_with_no_workflows:7932 invalid_repos:145\n",
            "Processed 17700 to 17800. repos_with_CI:9455 repos_with_network_error:200 repos_with_no_workflows:7999 invalid_repos:146\n",
            "Processed 17800 to 17900. repos_with_CI:9491 repos_with_network_error:200 repos_with_no_workflows:8062 invalid_repos:147\n",
            "Processed 17900 to 18000. repos_with_CI:9527 repos_with_network_error:200 repos_with_no_workflows:8125 invalid_repos:148\n",
            "Processed 18000 to 18100. repos_with_CI:9560 repos_with_network_error:200 repos_with_no_workflows:8191 invalid_repos:149\n",
            "Processed 18100 to 18200. repos_with_CI:9597 repos_with_network_error:200 repos_with_no_workflows:8254 invalid_repos:149\n",
            "Processed 18200 to 18300. repos_with_CI:9639 repos_with_network_error:200 repos_with_no_workflows:8311 invalid_repos:150\n",
            "Processed 18300 to 18400. repos_with_CI:9680 repos_with_network_error:200 repos_with_no_workflows:8370 invalid_repos:150\n",
            "Processed 18400 to 18500. repos_with_CI:9725 repos_with_network_error:200 repos_with_no_workflows:8424 invalid_repos:151\n",
            "Processed 18500 to 18600. repos_with_CI:9770 repos_with_network_error:200 repos_with_no_workflows:8478 invalid_repos:152\n",
            "Processed 18600 to 18700. repos_with_CI:9807 repos_with_network_error:200 repos_with_no_workflows:8536 invalid_repos:157\n",
            "Processed 18700 to 18800. repos_with_CI:9853 repos_with_network_error:200 repos_with_no_workflows:8589 invalid_repos:158\n",
            "Processed 18800 to 18900. repos_with_CI:9909 repos_with_network_error:200 repos_with_no_workflows:8628 invalid_repos:163\n",
            "Processed 18900 to 19000. repos_with_CI:9960 repos_with_network_error:200 repos_with_no_workflows:8673 invalid_repos:167\n",
            "Processed 19000 to 19100. repos_with_CI:10023 repos_with_network_error:200 repos_with_no_workflows:8710 invalid_repos:167\n",
            "Processed 19100 to 19200. repos_with_CI:10086 repos_with_network_error:200 repos_with_no_workflows:8743 invalid_repos:171\n",
            "Processed 19200 to 19300. repos_with_CI:10133 repos_with_network_error:200 repos_with_no_workflows:8792 invalid_repos:175\n",
            "Processed 19300 to 19400. repos_with_CI:10174 repos_with_network_error:200 repos_with_no_workflows:8846 invalid_repos:180\n",
            "Processed 19400 to 19500. repos_with_CI:10210 repos_with_network_error:200 repos_with_no_workflows:8908 invalid_repos:182\n",
            "Processed 19500 to 19600. repos_with_CI:10270 repos_with_network_error:200 repos_with_no_workflows:8943 invalid_repos:187\n",
            "Processed 19600 to 19700. repos_with_CI:10317 repos_with_network_error:200 repos_with_no_workflows:8991 invalid_repos:192\n",
            "Processed 19700 to 19800. repos_with_CI:10358 repos_with_network_error:200 repos_with_no_workflows:9048 invalid_repos:194\n",
            "Processed 19800 to 19900. repos_with_CI:10404 repos_with_network_error:200 repos_with_no_workflows:9094 invalid_repos:202\n",
            "Processed 19900 to 20000. repos_with_CI:10455 repos_with_network_error:200 repos_with_no_workflows:9138 invalid_repos:207\n",
            "Processed 20000 to 20100. repos_with_CI:10495 repos_with_network_error:200 repos_with_no_workflows:9198 invalid_repos:207\n",
            "Processed 20100 to 20200. repos_with_CI:10541 repos_with_network_error:200 repos_with_no_workflows:9252 invalid_repos:207\n",
            "Processed 20200 to 20300. repos_with_CI:10568 repos_with_network_error:200 repos_with_no_workflows:9319 invalid_repos:213\n",
            "Processed 20300 to 20400. repos_with_CI:10601 repos_with_network_error:200 repos_with_no_workflows:9384 invalid_repos:215\n",
            "Processed 20400 to 20500. repos_with_CI:10641 repos_with_network_error:200 repos_with_no_workflows:9441 invalid_repos:218\n",
            "Processed 20500 to 20600. repos_with_CI:10679 repos_with_network_error:200 repos_with_no_workflows:9498 invalid_repos:223\n",
            "Processed 20600 to 20700. repos_with_CI:10731 repos_with_network_error:200 repos_with_no_workflows:9542 invalid_repos:227\n",
            "Processed 20700 to 20800. repos_with_CI:10783 repos_with_network_error:200 repos_with_no_workflows:9586 invalid_repos:231\n",
            "Processed 20800 to 20900. repos_with_CI:10830 repos_with_network_error:200 repos_with_no_workflows:9634 invalid_repos:236\n",
            "Processed 20900 to 21000. repos_with_CI:10868 repos_with_network_error:200 repos_with_no_workflows:9690 invalid_repos:242\n",
            "Processed 21000 to 21100. repos_with_CI:10912 repos_with_network_error:200 repos_with_no_workflows:9739 invalid_repos:249\n",
            "Processed 21100 to 21200. repos_with_CI:10959 repos_with_network_error:200 repos_with_no_workflows:9787 invalid_repos:254\n",
            "Processed 21200 to 21300. repos_with_CI:10995 repos_with_network_error:200 repos_with_no_workflows:9845 invalid_repos:260\n",
            "Processed 21300 to 21400. repos_with_CI:11026 repos_with_network_error:200 repos_with_no_workflows:9910 invalid_repos:264\n",
            "Processed 21400 to 21500. repos_with_CI:11058 repos_with_network_error:200 repos_with_no_workflows:9974 invalid_repos:268\n",
            "Processed 21500 to 21600. repos_with_CI:11093 repos_with_network_error:200 repos_with_no_workflows:10038 invalid_repos:269\n",
            "Processed 21600 to 21700. repos_with_CI:11130 repos_with_network_error:200 repos_with_no_workflows:10098 invalid_repos:272\n",
            "Processed 21700 to 21800. repos_with_CI:11173 repos_with_network_error:200 repos_with_no_workflows:10153 invalid_repos:274\n",
            "Processed 21800 to 21900. repos_with_CI:11218 repos_with_network_error:200 repos_with_no_workflows:10204 invalid_repos:278\n",
            "Processed 21900 to 22000. repos_with_CI:11248 repos_with_network_error:200 repos_with_no_workflows:10269 invalid_repos:283\n",
            "Processed 22000 to 22100. repos_with_CI:11279 repos_with_network_error:200 repos_with_no_workflows:10333 invalid_repos:288\n",
            "Processed 22100 to 22200. repos_with_CI:11309 repos_with_network_error:200 repos_with_no_workflows:10398 invalid_repos:293\n",
            "Processed 22200 to 22300. repos_with_CI:11349 repos_with_network_error:200 repos_with_no_workflows:10456 invalid_repos:295\n",
            "Processed 22300 to 22400. repos_with_CI:11394 repos_with_network_error:200 repos_with_no_workflows:10507 invalid_repos:299\n",
            "Processed 22400 to 22500. repos_with_CI:11425 repos_with_network_error:200 repos_with_no_workflows:10569 invalid_repos:306\n",
            "Processed 22500 to 22600. repos_with_CI:11458 repos_with_network_error:200 repos_with_no_workflows:10633 invalid_repos:309\n",
            "Processed 22600 to 22700. repos_with_CI:11480 repos_with_network_error:200 repos_with_no_workflows:10707 invalid_repos:313\n",
            "Processed 22700 to 22800. repos_with_CI:11521 repos_with_network_error:200 repos_with_no_workflows:10764 invalid_repos:315\n",
            "Processed 22800 to 22900. repos_with_CI:11549 repos_with_network_error:200 repos_with_no_workflows:10832 invalid_repos:319\n",
            "Processed 22900 to 23000. repos_with_CI:11584 repos_with_network_error:200 repos_with_no_workflows:10890 invalid_repos:326\n",
            "Processed 23000 to 23100. repos_with_CI:11617 repos_with_network_error:200 repos_with_no_workflows:10952 invalid_repos:331\n",
            "Processed 23100 to 23200. repos_with_CI:11650 repos_with_network_error:200 repos_with_no_workflows:11013 invalid_repos:337\n",
            "Processed 23200 to 23300. repos_with_CI:11684 repos_with_network_error:200 repos_with_no_workflows:11075 invalid_repos:341\n",
            "Processed 23300 to 23400. repos_with_CI:11723 repos_with_network_error:200 repos_with_no_workflows:11132 invalid_repos:345\n",
            "Processed 23400 to 23500. repos_with_CI:11755 repos_with_network_error:200 repos_with_no_workflows:11197 invalid_repos:348\n",
            "Processed 23500 to 23600. repos_with_CI:11785 repos_with_network_error:200 repos_with_no_workflows:11260 invalid_repos:355\n",
            "Processed 23600 to 23700. repos_with_CI:11808 repos_with_network_error:200 repos_with_no_workflows:11326 invalid_repos:366\n",
            "Processed 23700 to 23800. repos_with_CI:11836 repos_with_network_error:200 repos_with_no_workflows:11392 invalid_repos:372\n",
            "Processed 23800 to 23900. repos_with_CI:11867 repos_with_network_error:200 repos_with_no_workflows:11455 invalid_repos:378\n",
            "Processed 23900 to 24000. repos_with_CI:11894 repos_with_network_error:200 repos_with_no_workflows:11522 invalid_repos:384\n",
            "Processed 24000 to 24100. repos_with_CI:11931 repos_with_network_error:200 repos_with_no_workflows:11578 invalid_repos:391\n",
            "Processed 24100 to 24200. repos_with_CI:11951 repos_with_network_error:200 repos_with_no_workflows:11649 invalid_repos:400\n",
            "Processed 24200 to 24300. repos_with_CI:11985 repos_with_network_error:200 repos_with_no_workflows:11704 invalid_repos:411\n",
            "Processed 24300 to 24400. repos_with_CI:12013 repos_with_network_error:200 repos_with_no_workflows:11765 invalid_repos:422\n",
            "Processed 24400 to 24500. repos_with_CI:12052 repos_with_network_error:200 repos_with_no_workflows:11822 invalid_repos:426\n",
            "Processed 24500 to 24600. repos_with_CI:12083 repos_with_network_error:200 repos_with_no_workflows:11888 invalid_repos:429\n",
            "Processed 24600 to 24612. repos_with_CI:12085 repos_with_network_error:200 repos_with_no_workflows:11898 invalid_repos:429\n"
          ]
        }
      ],
      "source": [
        "start = 0\n",
        "batch_size = 100\n",
        "limit = len(repos_from_csv)\n",
        "\n",
        "repos_with_CI = set()\n",
        "invalid_repos = set()\n",
        "repos_with_no_workflows = set()\n",
        "repos_with_network_error = set()\n",
        "\n",
        "for start in range(start, limit, batch_size):\n",
        "    end = min(start + batch_size, limit)\n",
        "    check_workflows(repos_from_csv, start, end)\n",
        "    print(f\"Processed {start} to {end}. repos_with_CI:{len(repos_with_CI)} repos_with_network_error:{len(repos_with_network_error)} repos_with_no_workflows:{len(repos_with_no_workflows)} invalid_repos:{len(invalid_repos)}\")\n",
        "    time.sleep(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ALEpLHlNCZGv"
      },
      "outputs": [],
      "source": [
        "# Save each set to a separate file\n",
        "def save_repo_names_to_file():\n",
        "    with open(\"files/repos_with_CI.txt\", \"w\") as f:\n",
        "        for repo in (repos_with_CI):\n",
        "            f.write(repo + \"\\n\")\n",
        "\n",
        "    with open(\"files/repos_with_no_workflows.txt\", \"w\") as f:\n",
        "        for repo in (repos_with_no_workflows):\n",
        "            f.write(repo + \"\\n\")\n",
        "\n",
        "    with open(\"files/invalid_repos.txt\", \"w\") as f:\n",
        "        for repo in (invalid_repos):\n",
        "            f.write(repo + \"\\n\")\n",
        "\n",
        "    with open(\"files/repos_with_network_error.txt\", \"w\") as f:\n",
        "        for repo in (repos_with_network_error):\n",
        "            f.write(repo + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_repo_names_to_file()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49-8mop-C-vl",
        "outputId": "ca310106-dc47-4564-97e2-620696d68e71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "repos_with_CI:12215, repos_with_network_error: 0\n"
          ]
        }
      ],
      "source": [
        "# retrying repos with network error\n",
        "unchecked_repos = []\n",
        "for error_repo in repos_with_network_error:\n",
        "    owner, repo_name = error_repo.split('/', 1)\n",
        "    unchecked_repos.append({\n",
        "        \"owner\": owner.strip(),\n",
        "        \"name\": repo_name.strip(),\n",
        "        \"default_branch\": repo_name_to_branch[f\"{owner.strip()}/{repo_name.strip()}\"].strip()\n",
        "    })\n",
        "\n",
        "batch_size = 100\n",
        "repos_with_network_error = set()\n",
        "\n",
        "for start in range(0, len(unchecked_repos), batch_size):\n",
        "    end = min(start + batch_size, limit)\n",
        "    check_workflows(unchecked_repos, start, end)\n",
        "\n",
        "print(f'repos_with_CI:{len(repos_with_CI)}, repos_with_network_error: {len(repos_with_network_error)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "-4TDGazOOkyX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "repos_with_CI:12215 repos_with_network_error:0 repos_with_no_workflows:11965 invalid_repos:432\n"
          ]
        }
      ],
      "source": [
        "save_repo_names_to_file()\n",
        "print(f\"repos_with_CI:{len(repos_with_CI)} repos_with_network_error:{len(repos_with_network_error)} repos_with_no_workflows:{len(repos_with_no_workflows)} invalid_repos:{len(invalid_repos)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "repos_with_CI:12215\n"
          ]
        }
      ],
      "source": [
        "# read from saved files\n",
        "repos_list_with_CI = []\n",
        "\n",
        "with open(\"files/repos_with_CI.txt\", \"r\") as file:\n",
        "    for line in file:\n",
        "        repos_list_with_CI.append(line.lower())\n",
        "\n",
        "print(f\"repos_with_CI:{len(repos_list_with_CI)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_query_for_download_workflows(start, end, repos_list):\n",
        "    query_parts = []\n",
        "    for idx, repo_full in enumerate(repos_list[start:end]):\n",
        "        owner, name = [x.strip() for x in repo_full.split(\"/\")]\n",
        "        query_parts.append(f\"\"\"\n",
        "        repo{idx}: repository(owner: \"{owner}\", name: \"{name}\") {{\n",
        "            object(expression: \"HEAD:.github/workflows\") {{\n",
        "                ... on Tree {{\n",
        "                    entries {{\n",
        "                        name\n",
        "                        type\n",
        "                        object {{\n",
        "                            ... on Blob {{\n",
        "                                text\n",
        "                            }}\n",
        "                        }}\n",
        "                    }}\n",
        "                }}\n",
        "            }}\n",
        "        }}\n",
        "        \"\"\")\n",
        "    return \"query {\\n\" + \"\\n\".join(query_parts) + \"\\n}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def download_workflows(repos_list):\n",
        "    start = 0\n",
        "    batch_size = 50\n",
        "    limit = len(repos_list)\n",
        "    sleep_time = 3\n",
        "    workflow_files = \"dummy\"\n",
        "\n",
        "    for start in range(start, limit, batch_size):\n",
        "        time.sleep(sleep_time)  # To respect rate limits\n",
        "        print(f\"Processing chunk {start} to {min(start + batch_size, limit)}\")\n",
        "        try:\n",
        "            end = min(start + batch_size, limit)\n",
        "            query = build_query_for_download_workflows(start, end, repos_list)\n",
        "            response = requests.post(\n",
        "                \"https://api.github.com/graphql\",\n",
        "                json={\"query\": query},\n",
        "                headers=HEADERS,\n",
        "                timeout=30  # Optional: avoid hanging\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json().get(\"data\", {})\n",
        "\n",
        "            for idx, repo_full in enumerate(repos_list[start:end]):\n",
        "                owner, repo_name = repo_full.split(\"/\")\n",
        "                repo_key = f\"repo{idx}\"\n",
        "\n",
        "                if(data.get(repo_key, {}) is None or data.get(repo_key, {}).get(\"object\") is None):\n",
        "                    print(f\"Data missing {repo_full}\")\n",
        "                    continue\n",
        "\n",
        "                entries = data.get(repo_key, {}).get(\"object\", {}).get(\"entries\", [])\n",
        "\n",
        "                if not entries:\n",
        "                    print(f\"No workflows found in {repo_full}\")\n",
        "                    continue\n",
        "\n",
        "                save_folder = os.path.join(workflow_files, owner.strip(), repo_name.strip())\n",
        "                os.makedirs(save_folder, exist_ok=True)\n",
        "\n",
        "                for entry in entries:\n",
        "                    if entry[\"type\"] == \"blob\":\n",
        "                        filename = entry[\"name\"]\n",
        "                        content = entry[\"object\"][\"text\"]\n",
        "                        file_path = os.path.join(save_folder, filename)\n",
        "                        if content is not None:\n",
        "                            with open(file_path, \"w\") as f:\n",
        "                                f.write(content)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing chunk {start}-{end}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing chunk 0 to 50\n",
            "Processing chunk 50 to 100\n",
            "Processing chunk 100 to 150\n",
            "Processing chunk 150 to 200\n",
            "Processing chunk 200 to 250\n",
            "Processing chunk 250 to 300\n",
            "Processing chunk 300 to 350\n",
            "Processing chunk 350 to 400\n",
            "Processing chunk 400 to 450\n",
            "Processing chunk 450 to 500\n",
            "Processing chunk 500 to 550\n",
            "Processing chunk 550 to 600\n",
            "Processing chunk 600 to 650\n",
            "Processing chunk 650 to 700\n",
            "Processing chunk 700 to 750\n",
            "Processing chunk 750 to 800\n",
            "Processing chunk 800 to 850\n",
            "Processing chunk 850 to 900\n",
            "Processing chunk 900 to 950\n",
            "Processing chunk 950 to 1000\n",
            "Processing chunk 1000 to 1050\n",
            "Processing chunk 1050 to 1100\n",
            "Processing chunk 1100 to 1150\n",
            "Processing chunk 1150 to 1200\n",
            "Processing chunk 1200 to 1250\n",
            "Data missing monsternone/tmall-miao\n",
            "\n",
            "Processing chunk 1250 to 1300\n",
            "Processing chunk 1300 to 1350\n",
            "Processing chunk 1350 to 1400\n",
            "Processing chunk 1400 to 1450\n",
            "Processing chunk 1450 to 1500\n",
            "Processing chunk 1500 to 1550\n",
            "Processing chunk 1550 to 1600\n",
            "Processing chunk 1600 to 1650\n",
            "Processing chunk 1650 to 1700\n",
            "Processing chunk 1700 to 1750\n",
            "Processing chunk 1750 to 1800\n",
            "Processing chunk 1800 to 1850\n",
            "Processing chunk 1850 to 1900\n",
            "Processing chunk 1900 to 1950\n",
            "Processing chunk 1950 to 2000\n",
            "Processing chunk 2000 to 2050\n",
            "Processing chunk 2050 to 2100\n",
            "Processing chunk 2100 to 2150\n",
            "Processing chunk 2150 to 2200\n",
            "Processing chunk 2200 to 2250\n",
            "Processing chunk 2250 to 2300\n",
            "Processing chunk 2300 to 2350\n",
            "Processing chunk 2350 to 2400\n",
            "Processing chunk 2400 to 2450\n",
            "Processing chunk 2450 to 2500\n",
            "Processing chunk 2500 to 2550\n",
            "Processing chunk 2550 to 2600\n",
            "Processing chunk 2600 to 2650\n",
            "Processing chunk 2650 to 2700\n",
            "Processing chunk 2700 to 2750\n",
            "Processing chunk 2750 to 2800\n",
            "Processing chunk 2800 to 2850\n",
            "Processing chunk 2850 to 2900\n",
            "Processing chunk 2900 to 2950\n",
            "Processing chunk 2950 to 3000\n",
            "Processing chunk 3000 to 3050\n",
            "Processing chunk 3050 to 3100\n",
            "Processing chunk 3100 to 3150\n",
            "Processing chunk 3150 to 3200\n",
            "Processing chunk 3200 to 3250\n",
            "Processing chunk 3250 to 3300\n",
            "Processing chunk 3300 to 3350\n",
            "Processing chunk 3350 to 3400\n",
            "Processing chunk 3400 to 3450\n",
            "Processing chunk 3450 to 3500\n",
            "Processing chunk 3500 to 3550\n",
            "Processing chunk 3550 to 3600\n",
            "Processing chunk 3600 to 3650\n",
            "Processing chunk 3650 to 3700\n",
            "Processing chunk 3700 to 3750\n",
            "Processing chunk 3750 to 3800\n",
            "Processing chunk 3800 to 3850\n",
            "Processing chunk 3850 to 3900\n",
            "Processing chunk 3900 to 3950\n",
            "Processing chunk 3950 to 4000\n",
            "Processing chunk 4000 to 4050\n",
            "Processing chunk 4050 to 4100\n",
            "Processing chunk 4100 to 4150\n",
            "Processing chunk 4150 to 4200\n",
            "Processing chunk 4200 to 4250\n",
            "Processing chunk 4250 to 4300\n",
            "Processing chunk 4300 to 4350\n",
            "Processing chunk 4350 to 4400\n",
            "Processing chunk 4400 to 4450\n",
            "Processing chunk 4450 to 4500\n",
            "Processing chunk 4500 to 4550\n",
            "Processing chunk 4550 to 4600\n",
            "Processing chunk 4600 to 4650\n",
            "Processing chunk 4650 to 4700\n",
            "Processing chunk 4700 to 4750\n",
            "Processing chunk 4750 to 4800\n",
            "Processing chunk 4800 to 4850\n",
            "Processing chunk 4850 to 4900\n",
            "Processing chunk 4900 to 4950\n",
            "Processing chunk 4950 to 5000\n",
            "Processing chunk 5000 to 5050\n",
            "Processing chunk 5050 to 5100\n",
            "Processing chunk 5100 to 5150\n",
            "Processing chunk 5150 to 5200\n",
            "Processing chunk 5200 to 5250\n",
            "Processing chunk 5250 to 5300\n",
            "Processing chunk 5300 to 5350\n",
            "Processing chunk 5350 to 5400\n",
            "Processing chunk 5400 to 5450\n",
            "Processing chunk 5450 to 5500\n",
            "Processing chunk 5500 to 5550\n",
            "Processing chunk 5550 to 5600\n",
            "Processing chunk 5600 to 5650\n",
            "Processing chunk 5650 to 5700\n",
            "Processing chunk 5700 to 5750\n",
            "Processing chunk 5750 to 5800\n",
            "Processing chunk 5800 to 5850\n",
            "Processing chunk 5850 to 5900\n",
            "Processing chunk 5900 to 5950\n",
            "Data missing hjyssg/shigureader\n",
            "\n",
            "Processing chunk 5950 to 6000\n",
            "Processing chunk 6000 to 6050\n",
            "Processing chunk 6050 to 6100\n",
            "Processing chunk 6100 to 6150\n",
            "Processing chunk 6150 to 6200\n",
            "Processing chunk 6200 to 6250\n",
            "Processing chunk 6250 to 6300\n",
            "Processing chunk 6300 to 6350\n",
            "Processing chunk 6350 to 6400\n",
            "Processing chunk 6400 to 6450\n",
            "Processing chunk 6450 to 6500\n",
            "Processing chunk 6500 to 6550\n",
            "Processing chunk 6550 to 6600\n",
            "Processing chunk 6600 to 6650\n",
            "Processing chunk 6650 to 6700\n",
            "Processing chunk 6700 to 6750\n",
            "Processing chunk 6750 to 6800\n",
            "Processing chunk 6800 to 6850\n",
            "Processing chunk 6850 to 6900\n",
            "Processing chunk 6900 to 6950\n",
            "Processing chunk 6950 to 7000\n",
            "Processing chunk 7000 to 7050\n",
            "Processing chunk 7050 to 7100\n",
            "Processing chunk 7100 to 7150\n",
            "Processing chunk 7150 to 7200\n",
            "Processing chunk 7200 to 7250\n",
            "Processing chunk 7250 to 7300\n",
            "Processing chunk 7300 to 7350\n",
            "Processing chunk 7350 to 7400\n",
            "Processing chunk 7400 to 7450\n",
            "Processing chunk 7450 to 7500\n",
            "Processing chunk 7500 to 7550\n",
            "Processing chunk 7550 to 7600\n",
            "Processing chunk 7600 to 7650\n",
            "Processing chunk 7650 to 7700\n",
            "Processing chunk 7700 to 7750\n",
            "Processing chunk 7750 to 7800\n",
            "Processing chunk 7800 to 7850\n",
            "Processing chunk 7850 to 7900\n",
            "Processing chunk 7900 to 7950\n",
            "Processing chunk 7950 to 8000\n",
            "Processing chunk 8000 to 8050\n",
            "Processing chunk 8050 to 8100\n",
            "Processing chunk 8100 to 8150\n",
            "Processing chunk 8150 to 8200\n",
            "Processing chunk 8200 to 8250\n",
            "Data missing jayofelony/pwnagotchi\n",
            "\n",
            "Processing chunk 8250 to 8300\n",
            "Processing chunk 8300 to 8350\n",
            "Processing chunk 8350 to 8400\n",
            "Processing chunk 8400 to 8450\n",
            "Processing chunk 8450 to 8500\n",
            "Processing chunk 8500 to 8550\n",
            "Processing chunk 8550 to 8600\n",
            "Processing chunk 8600 to 8650\n",
            "Processing chunk 8650 to 8700\n",
            "Processing chunk 8700 to 8750\n",
            "Processing chunk 8750 to 8800\n",
            "Processing chunk 8800 to 8850\n",
            "Processing chunk 8850 to 8900\n",
            "Processing chunk 8900 to 8950\n",
            "Processing chunk 8950 to 9000\n",
            "Processing chunk 9000 to 9050\n",
            "Processing chunk 9050 to 9100\n",
            "Processing chunk 9100 to 9150\n",
            "Processing chunk 9150 to 9200\n",
            "Processing chunk 9200 to 9250\n",
            "Processing chunk 9250 to 9300\n",
            "Processing chunk 9300 to 9350\n",
            "Processing chunk 9350 to 9400\n",
            "Processing chunk 9400 to 9450\n",
            "Processing chunk 9450 to 9500\n",
            "Processing chunk 9500 to 9550\n",
            "Processing chunk 9550 to 9600\n",
            "Processing chunk 9600 to 9650\n",
            "Processing chunk 9650 to 9700\n",
            "Processing chunk 9700 to 9750\n",
            "Processing chunk 9750 to 9800\n",
            "Processing chunk 9800 to 9850\n",
            "Processing chunk 9850 to 9900\n",
            "Processing chunk 9900 to 9950\n",
            "Processing chunk 9950 to 10000\n",
            "Processing chunk 10000 to 10050\n",
            "Processing chunk 10050 to 10100\n",
            "Processing chunk 10100 to 10150\n",
            "Processing chunk 10150 to 10200\n",
            "Processing chunk 10200 to 10250\n",
            "Processing chunk 10250 to 10300\n",
            "Processing chunk 10300 to 10350\n",
            "Processing chunk 10350 to 10400\n",
            "Processing chunk 10400 to 10450\n",
            "Processing chunk 10450 to 10500\n",
            "Processing chunk 10500 to 10550\n",
            "Processing chunk 10550 to 10600\n",
            "Processing chunk 10600 to 10650\n",
            "Processing chunk 10650 to 10700\n",
            "Processing chunk 10700 to 10750\n",
            "Processing chunk 10750 to 10800\n",
            "Processing chunk 10800 to 10850\n",
            "Processing chunk 10850 to 10900\n",
            "Processing chunk 10900 to 10950\n",
            "Processing chunk 10950 to 11000\n",
            "Processing chunk 11000 to 11050\n",
            "Processing chunk 11050 to 11100\n",
            "Processing chunk 11100 to 11150\n",
            "Processing chunk 11150 to 11200\n",
            "Processing chunk 11200 to 11250\n",
            "Processing chunk 11250 to 11300\n",
            "Processing chunk 11300 to 11350\n",
            "Processing chunk 11350 to 11400\n",
            "Processing chunk 11400 to 11450\n",
            "Processing chunk 11450 to 11500\n",
            "Processing chunk 11500 to 11550\n",
            "Processing chunk 11550 to 11600\n",
            "Processing chunk 11600 to 11650\n",
            "Data missing nulldev/spendenr-ai-d\n",
            "\n",
            "Processing chunk 11650 to 11700\n",
            "Processing chunk 11700 to 11750\n",
            "Processing chunk 11750 to 11800\n",
            "Processing chunk 11800 to 11850\n",
            "Processing chunk 11850 to 11900\n",
            "Data missing cpinitiative/ide\n",
            "\n",
            "Processing chunk 11900 to 11950\n",
            "Processing chunk 11950 to 12000\n",
            "Processing chunk 12000 to 12050\n",
            "Processing chunk 12050 to 12100\n",
            "Processing chunk 12100 to 12150\n",
            "Processing chunk 12150 to 12200\n",
            "Processing chunk 12200 to 12215\n"
          ]
        }
      ],
      "source": [
        "download_workflows(repos_list_with_CI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 267,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Total: 12210\n"
          ]
        }
      ],
      "source": [
        "def total_downloaded_workflows(root_folder):\n",
        "    result = []\n",
        "    for parent in os.listdir(root_folder):\n",
        "        parent_path = os.path.join(root_folder, parent)\n",
        "        if os.path.isdir(parent_path):\n",
        "            for child in os.listdir(parent_path):\n",
        "                child_path = os.path.join(parent_path, child)\n",
        "                if os.path.isdir(child_path):\n",
        "                    result.append(f\"{parent}/{child}\".lower().strip())\n",
        "    return result\n",
        "\n",
        "# Example usage:\n",
        "root = \"workflow_files\"\n",
        "total_downloaded_workflow_list = total_downloaded_workflows(root)\n",
        "\n",
        "print(\"\\nTotal:\", len(total_downloaded_workflow_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_unretrieved_repos():\n",
        "    unretrieved_repos_list = []\n",
        "    for r in repos_list_with_CI:\n",
        "        if r not in total_downloaded_workflow_list:\n",
        "            unretrieved_repos_list.append(r)\n",
        "    \n",
        "    return unretrieved_repos_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "unretrieved_repos_list = get_unretrieved_repos()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing chunk 0 to 5\n",
            "Data missing monsternone/tmall-miao\n",
            "\n",
            "Data missing hjyssg/shigureader\n",
            "\n",
            "Data missing jayofelony/pwnagotchi\n",
            "\n",
            "Data missing nulldev/spendenr-ai-d\n",
            "\n",
            "Data missing cpinitiative/ide\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download unretrieved repos: for network issues or other issues\n",
        "download_workflows(unretrieved_repos_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "metadata": {},
      "outputs": [],
      "source": [
        "ci_datas = []\n",
        "repos_with_missing_ci_datas = []\n",
        "\n",
        "def get_ci_data():\n",
        "    index = 0\n",
        "    for repo in total_downloaded_workflow_list:\n",
        "        \n",
        "        index += 1\n",
        "        if(index%20 == 0):\n",
        "            print(f\"Processing {index} out of {len(total_downloaded_workflow_list)}\")\n",
        "        \n",
        "        try:\n",
        "            time.sleep(1)\n",
        "\n",
        "            OWNER, REPO = repo.split('/', 1)\n",
        "            BRANCH = repo_name_to_branch[f\"{OWNER}/{REPO}\".lower()]\n",
        "\n",
        "            # Get workflow runs directly (this gets the latest commit info too)\n",
        "            runs_url = f\"https://api.github.com/repos/{OWNER}/{REPO}/actions/runs\"\n",
        "            params = {\"per_page\": 1} # Ignoring main/master branch, as latest commit could be in any branch\n",
        "\n",
        "            runs_resp = requests.get(runs_url, headers=HEADERS, params=params)\n",
        "            runs_resp.raise_for_status()\n",
        "            runs_data = runs_resp.json()\n",
        "\n",
        "            if \"workflow_runs\" in runs_data and runs_data[\"workflow_runs\"]:\n",
        "                run = runs_data[\"workflow_runs\"][0]\n",
        "                latest_sha = run.get(\"head_sha\")  # Get SHA from workflow run\n",
        "\n",
        "                ci_datas.append({\n",
        "                    \"repo\": f\"{OWNER}/{REPO}\",\n",
        "                    \"branch\": run.get(\"head_branch\"),\n",
        "                    \"default_branch\": BRANCH,\n",
        "                    \"commit\": latest_sha,\n",
        "                    \"workflow_name\": run.get(\"name\"),\n",
        "                    \"run_id\": run.get(\"id\"),\n",
        "                    \"status\": run.get(\"status\"),\n",
        "                    \"conclusion\": run.get(\"conclusion\"),\n",
        "                    \"event\": run.get(\"event\"),\n",
        "                    \"url\": run.get(\"html_url\"),\n",
        "                    \"start_time\": run.get(\"run_started_at\"),\n",
        "                    \"end_time\": run.get(\"updated_at\"),\n",
        "                    \"path\": run.get(\"path\"),\n",
        "                })\n",
        "\n",
        "            else:\n",
        "                repos_with_missing_ci_datas.append(f\"{OWNER}/{REPO}\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {OWNER}/{REPO}: {e}\")\n",
        "            repos_with_missing_ci_datas.append(f\"{OWNER}/{REPO}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 268,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 20 out of 12210\n",
            "Processing 40 out of 12210\n",
            "Processing 60 out of 12210\n",
            "Processing 80 out of 12210\n",
            "Processing 100 out of 12210\n",
            "Processing 120 out of 12210\n",
            "Processing 140 out of 12210\n",
            "Processing 160 out of 12210\n",
            "Processing 180 out of 12210\n",
            "Processing 200 out of 12210\n",
            "Processing 220 out of 12210\n",
            "Processing 240 out of 12210\n",
            "Processing 260 out of 12210\n",
            "Processing 280 out of 12210\n",
            "Processing 300 out of 12210\n",
            "Processing 320 out of 12210\n",
            "Processing 340 out of 12210\n",
            "Processing 360 out of 12210\n",
            "Processing 380 out of 12210\n",
            "Processing 400 out of 12210\n",
            "Processing 420 out of 12210\n",
            "Processing 440 out of 12210\n",
            "Processing 460 out of 12210\n",
            "Processing 480 out of 12210\n",
            "Processing 500 out of 12210\n",
            "Processing 520 out of 12210\n",
            "Processing 540 out of 12210\n",
            "Processing 560 out of 12210\n",
            "Processing 580 out of 12210\n",
            "Processing 600 out of 12210\n",
            "Processing 620 out of 12210\n",
            "Processing 640 out of 12210\n",
            "Processing 660 out of 12210\n",
            "Processing 680 out of 12210\n",
            "Processing 700 out of 12210\n",
            "Processing 720 out of 12210\n",
            "Processing 740 out of 12210\n",
            "Processing 760 out of 12210\n",
            "Processing 780 out of 12210\n",
            "Processing 800 out of 12210\n",
            "Processing 820 out of 12210\n",
            "Processing 840 out of 12210\n",
            "Processing 860 out of 12210\n",
            "Processing 880 out of 12210\n",
            "Processing 900 out of 12210\n",
            "Processing 920 out of 12210\n",
            "Processing 940 out of 12210\n",
            "Processing 960 out of 12210\n",
            "Processing 980 out of 12210\n",
            "Processing 1000 out of 12210\n",
            "Error processing sr2echa/thottathukiduven: 404 Client Error: Not Found for url: https://api.github.com/repos/sr2echa/thottathukiduven/actions/runs?per_page=1\n",
            "Processing 1020 out of 12210\n",
            "Processing 1040 out of 12210\n",
            "Processing 1060 out of 12210\n",
            "Processing 1080 out of 12210\n",
            "Processing 1100 out of 12210\n",
            "Processing 1120 out of 12210\n",
            "Processing 1140 out of 12210\n",
            "Processing 1160 out of 12210\n",
            "Processing 1180 out of 12210\n",
            "Processing 1200 out of 12210\n",
            "Processing 1220 out of 12210\n",
            "Processing 1240 out of 12210\n",
            "Processing 1260 out of 12210\n",
            "Processing 1280 out of 12210\n",
            "Processing 1300 out of 12210\n",
            "Processing 1320 out of 12210\n",
            "Processing 1340 out of 12210\n",
            "Processing 1360 out of 12210\n",
            "Processing 1380 out of 12210\n",
            "Processing 1400 out of 12210\n",
            "Processing 1420 out of 12210\n",
            "Processing 1440 out of 12210\n",
            "Processing 1460 out of 12210\n",
            "Processing 1480 out of 12210\n",
            "Processing 1500 out of 12210\n",
            "Processing 1520 out of 12210\n",
            "Processing 1540 out of 12210\n",
            "Processing 1560 out of 12210\n",
            "Processing 1580 out of 12210\n",
            "Processing 1600 out of 12210\n",
            "Processing 1620 out of 12210\n",
            "Processing 1640 out of 12210\n",
            "Processing 1660 out of 12210\n",
            "Processing 1680 out of 12210\n",
            "Processing 1700 out of 12210\n",
            "Processing 1720 out of 12210\n",
            "Processing 1740 out of 12210\n",
            "Processing 1760 out of 12210\n",
            "Processing 1780 out of 12210\n",
            "Processing 1800 out of 12210\n",
            "Processing 1820 out of 12210\n",
            "Processing 1840 out of 12210\n",
            "Processing 1860 out of 12210\n",
            "Processing 1880 out of 12210\n",
            "Processing 1900 out of 12210\n",
            "Processing 1920 out of 12210\n",
            "Processing 1940 out of 12210\n",
            "Processing 1960 out of 12210\n",
            "Processing 1980 out of 12210\n",
            "Processing 2000 out of 12210\n",
            "Processing 2020 out of 12210\n",
            "Processing 2040 out of 12210\n",
            "Processing 2060 out of 12210\n",
            "Processing 2080 out of 12210\n",
            "Processing 2100 out of 12210\n",
            "Processing 2120 out of 12210\n",
            "Processing 2140 out of 12210\n",
            "Processing 2160 out of 12210\n",
            "Processing 2180 out of 12210\n",
            "Processing 2200 out of 12210\n",
            "Processing 2220 out of 12210\n",
            "Processing 2240 out of 12210\n",
            "Processing 2260 out of 12210\n",
            "Processing 2280 out of 12210\n",
            "Processing 2300 out of 12210\n",
            "Processing 2320 out of 12210\n",
            "Processing 2340 out of 12210\n",
            "Processing 2360 out of 12210\n",
            "Processing 2380 out of 12210\n",
            "Processing 2400 out of 12210\n",
            "Processing 2420 out of 12210\n",
            "Processing 2440 out of 12210\n",
            "Processing 2460 out of 12210\n",
            "Processing 2480 out of 12210\n",
            "Processing 2500 out of 12210\n",
            "Processing 2520 out of 12210\n",
            "Processing 2540 out of 12210\n",
            "Processing 2560 out of 12210\n",
            "Processing 2580 out of 12210\n",
            "Processing 2600 out of 12210\n",
            "Processing 2620 out of 12210\n",
            "Processing 2640 out of 12210\n",
            "Processing 2660 out of 12210\n",
            "Processing 2680 out of 12210\n",
            "Processing 2700 out of 12210\n",
            "Processing 2720 out of 12210\n",
            "Processing 2740 out of 12210\n",
            "Processing 2760 out of 12210\n",
            "Processing 2780 out of 12210\n",
            "Processing 2800 out of 12210\n",
            "Processing 2820 out of 12210\n",
            "Processing 2840 out of 12210\n",
            "Processing 2860 out of 12210\n",
            "Processing 2880 out of 12210\n",
            "Processing 2900 out of 12210\n",
            "Processing 2920 out of 12210\n",
            "Processing 2940 out of 12210\n",
            "Processing 2960 out of 12210\n",
            "Processing 2980 out of 12210\n",
            "Processing 3000 out of 12210\n",
            "Processing 3020 out of 12210\n",
            "Processing 3040 out of 12210\n",
            "Processing 3060 out of 12210\n",
            "Processing 3080 out of 12210\n",
            "Processing 3100 out of 12210\n",
            "Processing 3120 out of 12210\n",
            "Processing 3140 out of 12210\n",
            "Processing 3160 out of 12210\n",
            "Processing 3180 out of 12210\n",
            "Processing 3200 out of 12210\n",
            "Processing 3220 out of 12210\n",
            "Processing 3240 out of 12210\n",
            "Processing 3260 out of 12210\n",
            "Processing 3280 out of 12210\n",
            "Processing 3300 out of 12210\n",
            "Processing 3320 out of 12210\n",
            "Processing 3340 out of 12210\n",
            "Processing 3360 out of 12210\n",
            "Processing 3380 out of 12210\n",
            "Processing 3400 out of 12210\n",
            "Processing 3420 out of 12210\n",
            "Processing 3440 out of 12210\n",
            "Processing 3460 out of 12210\n",
            "Processing 3480 out of 12210\n",
            "Processing 3500 out of 12210\n",
            "Processing 3520 out of 12210\n",
            "Processing 3540 out of 12210\n",
            "Processing 3560 out of 12210\n",
            "Processing 3580 out of 12210\n",
            "Processing 3600 out of 12210\n",
            "Processing 3620 out of 12210\n",
            "Processing 3640 out of 12210\n",
            "Processing 3660 out of 12210\n",
            "Processing 3680 out of 12210\n",
            "Processing 3700 out of 12210\n",
            "Processing 3720 out of 12210\n",
            "Processing 3740 out of 12210\n",
            "Processing 3760 out of 12210\n",
            "Processing 3780 out of 12210\n",
            "Processing 3800 out of 12210\n",
            "Processing 3820 out of 12210\n",
            "Processing 3840 out of 12210\n",
            "Processing 3860 out of 12210\n",
            "Processing 3880 out of 12210\n",
            "Processing 3900 out of 12210\n",
            "Processing 3920 out of 12210\n",
            "Processing 3940 out of 12210\n",
            "Processing 3960 out of 12210\n",
            "Processing 3980 out of 12210\n",
            "Processing 4000 out of 12210\n",
            "Processing 4020 out of 12210\n",
            "Processing 4040 out of 12210\n",
            "Processing 4060 out of 12210\n",
            "Processing 4080 out of 12210\n",
            "Processing 4100 out of 12210\n",
            "Processing 4120 out of 12210\n",
            "Processing 4140 out of 12210\n",
            "Processing 4160 out of 12210\n",
            "Processing 4180 out of 12210\n",
            "Processing 4200 out of 12210\n",
            "Processing 4220 out of 12210\n",
            "Processing 4240 out of 12210\n",
            "Processing 4260 out of 12210\n",
            "Processing 4280 out of 12210\n",
            "Processing 4300 out of 12210\n",
            "Processing 4320 out of 12210\n",
            "Processing 4340 out of 12210\n",
            "Processing 4360 out of 12210\n",
            "Processing 4380 out of 12210\n",
            "Processing 4400 out of 12210\n",
            "Processing 4420 out of 12210\n",
            "Processing 4440 out of 12210\n",
            "Processing 4460 out of 12210\n",
            "Processing 4480 out of 12210\n",
            "Processing 4500 out of 12210\n",
            "Processing 4520 out of 12210\n",
            "Processing 4540 out of 12210\n",
            "Processing 4560 out of 12210\n",
            "Processing 4580 out of 12210\n",
            "Processing 4600 out of 12210\n",
            "Processing 4620 out of 12210\n",
            "Processing 4640 out of 12210\n",
            "Processing 4660 out of 12210\n",
            "Processing 4680 out of 12210\n",
            "Processing 4700 out of 12210\n",
            "Processing 4720 out of 12210\n",
            "Processing 4740 out of 12210\n",
            "Processing 4760 out of 12210\n",
            "Processing 4780 out of 12210\n",
            "Processing 4800 out of 12210\n",
            "Processing 4820 out of 12210\n",
            "Processing 4840 out of 12210\n",
            "Processing 4860 out of 12210\n",
            "Processing 4880 out of 12210\n",
            "Processing 4900 out of 12210\n",
            "Processing 4920 out of 12210\n",
            "Processing 4940 out of 12210\n",
            "Processing 4960 out of 12210\n",
            "Processing 4980 out of 12210\n",
            "Processing 5000 out of 12210\n",
            "Processing 5020 out of 12210\n",
            "Processing 5040 out of 12210\n",
            "Processing 5060 out of 12210\n",
            "Processing 5080 out of 12210\n",
            "Processing 5100 out of 12210\n",
            "Processing 5120 out of 12210\n",
            "Processing 5140 out of 12210\n",
            "Processing 5160 out of 12210\n",
            "Processing 5180 out of 12210\n",
            "Processing 5200 out of 12210\n",
            "Processing 5220 out of 12210\n",
            "Processing 5240 out of 12210\n",
            "Processing 5260 out of 12210\n",
            "Processing 5280 out of 12210\n",
            "Processing 5300 out of 12210\n",
            "Processing 5320 out of 12210\n",
            "Processing 5340 out of 12210\n",
            "Processing 5360 out of 12210\n",
            "Processing 5380 out of 12210\n",
            "Processing 5400 out of 12210\n",
            "Processing 5420 out of 12210\n",
            "Processing 5440 out of 12210\n",
            "Processing 5460 out of 12210\n",
            "Processing 5480 out of 12210\n",
            "Processing 5500 out of 12210\n",
            "Processing 5520 out of 12210\n",
            "Processing 5540 out of 12210\n",
            "Processing 5560 out of 12210\n",
            "Processing 5580 out of 12210\n",
            "Processing 5600 out of 12210\n",
            "Processing 5620 out of 12210\n",
            "Processing 5640 out of 12210\n",
            "Processing 5660 out of 12210\n",
            "Processing 5680 out of 12210\n",
            "Processing 5700 out of 12210\n",
            "Processing 5720 out of 12210\n",
            "Processing 5740 out of 12210\n",
            "Processing 5760 out of 12210\n",
            "Processing 5780 out of 12210\n",
            "Processing 5800 out of 12210\n",
            "Processing 5820 out of 12210\n",
            "Processing 5840 out of 12210\n",
            "Processing 5860 out of 12210\n",
            "Processing 5880 out of 12210\n",
            "Processing 5900 out of 12210\n",
            "Processing 5920 out of 12210\n",
            "Processing 5940 out of 12210\n",
            "Processing 5960 out of 12210\n",
            "Processing 5980 out of 12210\n",
            "Processing 6000 out of 12210\n",
            "Processing 6020 out of 12210\n",
            "Processing 6040 out of 12210\n",
            "Processing 6060 out of 12210\n",
            "Processing 6080 out of 12210\n",
            "Processing 6100 out of 12210\n",
            "Processing 6120 out of 12210\n",
            "Processing 6140 out of 12210\n",
            "Processing 6160 out of 12210\n",
            "Processing 6180 out of 12210\n",
            "Processing 6200 out of 12210\n",
            "Processing 6220 out of 12210\n",
            "Processing 6240 out of 12210\n",
            "Processing 6260 out of 12210\n",
            "Processing 6280 out of 12210\n",
            "Processing 6300 out of 12210\n",
            "Processing 6320 out of 12210\n",
            "Processing 6340 out of 12210\n",
            "Processing 6360 out of 12210\n",
            "Processing 6380 out of 12210\n",
            "Processing 6400 out of 12210\n",
            "Processing 6420 out of 12210\n",
            "Processing 6440 out of 12210\n",
            "Processing 6460 out of 12210\n",
            "Processing 6480 out of 12210\n",
            "Processing 6500 out of 12210\n",
            "Processing 6520 out of 12210\n",
            "Processing 6540 out of 12210\n",
            "Processing 6560 out of 12210\n",
            "Processing 6580 out of 12210\n",
            "Processing 6600 out of 12210\n",
            "Processing 6620 out of 12210\n",
            "Processing 6640 out of 12210\n",
            "Processing 6660 out of 12210\n",
            "Processing 6680 out of 12210\n",
            "Processing 6700 out of 12210\n",
            "Processing 6720 out of 12210\n",
            "Processing 6740 out of 12210\n",
            "Processing 6760 out of 12210\n",
            "Processing 6780 out of 12210\n",
            "Processing 6800 out of 12210\n",
            "Processing 6820 out of 12210\n",
            "Processing 6840 out of 12210\n",
            "Processing 6860 out of 12210\n",
            "Processing 6880 out of 12210\n",
            "Processing 6900 out of 12210\n",
            "Processing 6920 out of 12210\n",
            "Processing 6940 out of 12210\n",
            "Processing 6960 out of 12210\n",
            "Processing 6980 out of 12210\n",
            "Processing 7000 out of 12210\n",
            "Processing 7020 out of 12210\n",
            "Processing 7040 out of 12210\n",
            "Processing 7060 out of 12210\n",
            "Processing 7080 out of 12210\n",
            "Processing 7100 out of 12210\n",
            "Processing 7120 out of 12210\n",
            "Processing 7140 out of 12210\n",
            "Processing 7160 out of 12210\n",
            "Processing 7180 out of 12210\n",
            "Processing 7200 out of 12210\n",
            "Processing 7220 out of 12210\n",
            "Processing 7240 out of 12210\n",
            "Processing 7260 out of 12210\n",
            "Processing 7280 out of 12210\n",
            "Processing 7300 out of 12210\n",
            "Processing 7320 out of 12210\n",
            "Processing 7340 out of 12210\n",
            "Processing 7360 out of 12210\n",
            "Processing 7380 out of 12210\n",
            "Processing 7400 out of 12210\n",
            "Processing 7420 out of 12210\n",
            "Processing 7440 out of 12210\n",
            "Processing 7460 out of 12210\n",
            "Processing 7480 out of 12210\n",
            "Processing 7500 out of 12210\n",
            "Processing 7520 out of 12210\n",
            "Processing 7540 out of 12210\n",
            "Processing 7560 out of 12210\n",
            "Processing 7580 out of 12210\n",
            "Processing 7600 out of 12210\n",
            "Processing 7620 out of 12210\n",
            "Processing 7640 out of 12210\n",
            "Processing 7660 out of 12210\n",
            "Processing 7680 out of 12210\n",
            "Processing 7700 out of 12210\n",
            "Processing 7720 out of 12210\n",
            "Processing 7740 out of 12210\n",
            "Processing 7760 out of 12210\n",
            "Processing 7780 out of 12210\n",
            "Processing 7800 out of 12210\n",
            "Processing 7820 out of 12210\n",
            "Processing 7840 out of 12210\n",
            "Processing 7860 out of 12210\n",
            "Processing 7880 out of 12210\n",
            "Processing 7900 out of 12210\n",
            "Processing 7920 out of 12210\n",
            "Processing 7940 out of 12210\n",
            "Processing 7960 out of 12210\n",
            "Processing 7980 out of 12210\n",
            "Processing 8000 out of 12210\n",
            "Processing 8020 out of 12210\n",
            "Processing 8040 out of 12210\n",
            "Processing 8060 out of 12210\n",
            "Processing 8080 out of 12210\n",
            "Processing 8100 out of 12210\n",
            "Processing 8120 out of 12210\n",
            "Processing 8140 out of 12210\n",
            "Processing 8160 out of 12210\n",
            "Processing 8180 out of 12210\n",
            "Processing 8200 out of 12210\n",
            "Processing 8220 out of 12210\n",
            "Processing 8240 out of 12210\n",
            "Processing 8260 out of 12210\n",
            "Processing 8280 out of 12210\n",
            "Processing 8300 out of 12210\n",
            "Processing 8320 out of 12210\n",
            "Processing 8340 out of 12210\n",
            "Processing 8360 out of 12210\n",
            "Processing 8380 out of 12210\n",
            "Processing 8400 out of 12210\n",
            "Processing 8420 out of 12210\n",
            "Processing 8440 out of 12210\n",
            "Processing 8460 out of 12210\n",
            "Processing 8480 out of 12210\n",
            "Processing 8500 out of 12210\n",
            "Processing 8520 out of 12210\n",
            "Processing 8540 out of 12210\n",
            "Processing 8560 out of 12210\n",
            "Processing 8580 out of 12210\n",
            "Processing 8600 out of 12210\n",
            "Processing 8620 out of 12210\n",
            "Processing 8640 out of 12210\n",
            "Processing 8660 out of 12210\n",
            "Processing 8680 out of 12210\n",
            "Processing 8700 out of 12210\n",
            "Processing 8720 out of 12210\n",
            "Processing 8740 out of 12210\n",
            "Processing 8760 out of 12210\n",
            "Processing 8780 out of 12210\n",
            "Processing 8800 out of 12210\n",
            "Processing 8820 out of 12210\n",
            "Processing 8840 out of 12210\n",
            "Processing 8860 out of 12210\n",
            "Processing 8880 out of 12210\n",
            "Processing 8900 out of 12210\n",
            "Processing 8920 out of 12210\n",
            "Processing 8940 out of 12210\n",
            "Processing 8960 out of 12210\n",
            "Processing 8980 out of 12210\n",
            "Processing 9000 out of 12210\n",
            "Processing 9020 out of 12210\n",
            "Processing 9040 out of 12210\n",
            "Processing 9060 out of 12210\n",
            "Processing 9080 out of 12210\n",
            "Processing 9100 out of 12210\n",
            "Processing 9120 out of 12210\n",
            "Processing 9140 out of 12210\n",
            "Processing 9160 out of 12210\n",
            "Processing 9180 out of 12210\n",
            "Processing 9200 out of 12210\n",
            "Processing 9220 out of 12210\n",
            "Processing 9240 out of 12210\n",
            "Processing 9260 out of 12210\n",
            "Processing 9280 out of 12210\n",
            "Processing 9300 out of 12210\n",
            "Processing 9320 out of 12210\n",
            "Processing 9340 out of 12210\n",
            "Processing 9360 out of 12210\n",
            "Processing 9380 out of 12210\n",
            "Processing 9400 out of 12210\n",
            "Processing 9420 out of 12210\n",
            "Processing 9440 out of 12210\n",
            "Processing 9460 out of 12210\n",
            "Processing 9480 out of 12210\n",
            "Processing 9500 out of 12210\n",
            "Processing 9520 out of 12210\n",
            "Processing 9540 out of 12210\n",
            "Processing 9560 out of 12210\n",
            "Processing 9580 out of 12210\n",
            "Processing 9600 out of 12210\n",
            "Processing 9620 out of 12210\n",
            "Processing 9640 out of 12210\n",
            "Processing 9660 out of 12210\n",
            "Processing 9680 out of 12210\n",
            "Processing 9700 out of 12210\n",
            "Processing 9720 out of 12210\n",
            "Processing 9740 out of 12210\n",
            "Processing 9760 out of 12210\n",
            "Processing 9780 out of 12210\n",
            "Processing 9800 out of 12210\n",
            "Error processing commaai/connect: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "Processing 9820 out of 12210\n",
            "Processing 9840 out of 12210\n",
            "Processing 9860 out of 12210\n",
            "Processing 9880 out of 12210\n",
            "Processing 9900 out of 12210\n",
            "Processing 9920 out of 12210\n",
            "Processing 9940 out of 12210\n",
            "Processing 9960 out of 12210\n",
            "Processing 9980 out of 12210\n",
            "Processing 10000 out of 12210\n",
            "Error processing erzaozi/waves-plugin: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "Processing 10020 out of 12210\n",
            "Processing 10040 out of 12210\n",
            "Processing 10060 out of 12210\n",
            "Processing 10080 out of 12210\n",
            "Processing 10100 out of 12210\n",
            "Processing 10120 out of 12210\n",
            "Processing 10140 out of 12210\n",
            "Processing 10160 out of 12210\n",
            "Processing 10180 out of 12210\n",
            "Processing 10200 out of 12210\n",
            "Processing 10220 out of 12210\n",
            "Processing 10240 out of 12210\n",
            "Processing 10260 out of 12210\n",
            "Processing 10280 out of 12210\n",
            "Processing 10300 out of 12210\n",
            "Processing 10320 out of 12210\n",
            "Processing 10340 out of 12210\n",
            "Processing 10360 out of 12210\n",
            "Processing 10380 out of 12210\n",
            "Processing 10400 out of 12210\n",
            "Processing 10420 out of 12210\n",
            "Processing 10440 out of 12210\n",
            "Processing 10460 out of 12210\n",
            "Processing 10480 out of 12210\n",
            "Processing 10500 out of 12210\n",
            "Processing 10520 out of 12210\n",
            "Processing 10540 out of 12210\n",
            "Processing 10560 out of 12210\n",
            "Processing 10580 out of 12210\n",
            "Processing 10600 out of 12210\n",
            "Processing 10620 out of 12210\n",
            "Processing 10640 out of 12210\n",
            "Processing 10660 out of 12210\n",
            "Processing 10680 out of 12210\n",
            "Processing 10700 out of 12210\n",
            "Processing 10720 out of 12210\n",
            "Processing 10740 out of 12210\n",
            "Processing 10760 out of 12210\n",
            "Processing 10780 out of 12210\n",
            "Processing 10800 out of 12210\n",
            "Processing 10820 out of 12210\n",
            "Processing 10840 out of 12210\n",
            "Error processing mampfes/ha_freeair_connect: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "Processing 10860 out of 12210\n",
            "Processing 10880 out of 12210\n",
            "Processing 10900 out of 12210\n",
            "Processing 10920 out of 12210\n",
            "Processing 10940 out of 12210\n",
            "Processing 10960 out of 12210\n",
            "Processing 10980 out of 12210\n",
            "Processing 11000 out of 12210\n",
            "Processing 11020 out of 12210\n",
            "Processing 11040 out of 12210\n",
            "Processing 11060 out of 12210\n",
            "Processing 11080 out of 12210\n",
            "Processing 11100 out of 12210\n",
            "Processing 11120 out of 12210\n",
            "Processing 11140 out of 12210\n",
            "Processing 11160 out of 12210\n",
            "Processing 11180 out of 12210\n",
            "Processing 11200 out of 12210\n",
            "Processing 11220 out of 12210\n",
            "Processing 11240 out of 12210\n",
            "Processing 11260 out of 12210\n",
            "Processing 11280 out of 12210\n",
            "Processing 11300 out of 12210\n",
            "Processing 11320 out of 12210\n",
            "Processing 11340 out of 12210\n",
            "Processing 11360 out of 12210\n",
            "Processing 11380 out of 12210\n",
            "Processing 11400 out of 12210\n",
            "Processing 11420 out of 12210\n",
            "Processing 11440 out of 12210\n",
            "Processing 11460 out of 12210\n",
            "Processing 11480 out of 12210\n",
            "Processing 11500 out of 12210\n",
            "Processing 11520 out of 12210\n",
            "Processing 11540 out of 12210\n",
            "Processing 11560 out of 12210\n",
            "Processing 11580 out of 12210\n",
            "Processing 11600 out of 12210\n",
            "Processing 11620 out of 12210\n",
            "Processing 11640 out of 12210\n",
            "Processing 11660 out of 12210\n",
            "Processing 11680 out of 12210\n",
            "Processing 11700 out of 12210\n",
            "Processing 11720 out of 12210\n",
            "Processing 11740 out of 12210\n",
            "Processing 11760 out of 12210\n",
            "Error processing yunnysunny/nodebook: HTTPSConnectionPool(host='api.github.com', port=443): Max retries exceeded with url: /repos/yunnysunny/nodebook/actions/runs?per_page=1 (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1028)')))\n",
            "Processing 11780 out of 12210\n",
            "Processing 11800 out of 12210\n",
            "Processing 11820 out of 12210\n",
            "Processing 11840 out of 12210\n",
            "Processing 11860 out of 12210\n",
            "Processing 11880 out of 12210\n",
            "Processing 11900 out of 12210\n",
            "Processing 11920 out of 12210\n",
            "Processing 11940 out of 12210\n",
            "Processing 11960 out of 12210\n",
            "Processing 11980 out of 12210\n",
            "Processing 12000 out of 12210\n",
            "Processing 12020 out of 12210\n",
            "Processing 12040 out of 12210\n",
            "Processing 12060 out of 12210\n",
            "Processing 12080 out of 12210\n",
            "Processing 12100 out of 12210\n",
            "Error processing ocaml-sf/learn-ocaml: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "Processing 12120 out of 12210\n",
            "Processing 12140 out of 12210\n",
            "Processing 12160 out of 12210\n",
            "Processing 12180 out of 12210\n",
            "Processing 12200 out of 12210\n"
          ]
        }
      ],
      "source": [
        "get_ci_data()\n",
        "\n",
        "df_ci_data = pd.DataFrame(ci_datas)\n",
        "\n",
        "\n",
        "df_ci_data.to_csv(\"files/ci_data.csv\", index=False)\n",
        "\n",
        "with open(\"files/repos_with_missing_ci_datas.txt\", \"w\") as f:\n",
        "    for repo in (repos_with_missing_ci_datas):\n",
        "        f.write(repo + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 264,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of repo with github workflow action: 11075\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>repo</th>\n",
              "      <th>branch</th>\n",
              "      <th>default_branch</th>\n",
              "      <th>commit</th>\n",
              "      <th>workflow_name</th>\n",
              "      <th>run_id</th>\n",
              "      <th>status</th>\n",
              "      <th>conclusion</th>\n",
              "      <th>event</th>\n",
              "      <th>url</th>\n",
              "      <th>start_time</th>\n",
              "      <th>end_time</th>\n",
              "      <th>runner_environment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>dvgis/dc-sdk</td>\n",
              "      <td>master</td>\n",
              "      <td>master</td>\n",
              "      <td>e810a4a289c04cbd66299fbf53d7e09f5ef7f873</td>\n",
              "      <td>build</td>\n",
              "      <td>17342172601</td>\n",
              "      <td>completed</td>\n",
              "      <td>success</td>\n",
              "      <td>push</td>\n",
              "      <td>https://github.com/dvt3d/dc-sdk/actions/runs/1...</td>\n",
              "      <td>2025-08-30T09:32:19Z</td>\n",
              "      <td>2025-08-30T09:34:17Z</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>chimoney/chimoney-community-projects</td>\n",
              "      <td>main</td>\n",
              "      <td>main</td>\n",
              "      <td>e27d3bc8a7211be50c265ed5b8353be85170a39f</td>\n",
              "      <td>PR Merge Badge Automation</td>\n",
              "      <td>16342288531</td>\n",
              "      <td>completed</td>\n",
              "      <td>skipped</td>\n",
              "      <td>pull_request</td>\n",
              "      <td>https://github.com/Chimoney/chimoney-community...</td>\n",
              "      <td>2025-07-17T10:09:03Z</td>\n",
              "      <td>2025-07-17T10:09:04Z</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>pony-house/client</td>\n",
              "      <td>dev</td>\n",
              "      <td>dev</td>\n",
              "      <td>bcb43f6e0a18f03e40dc058331a2b014ec367b0a</td>\n",
              "      <td>docker in /. - Update #1095030283</td>\n",
              "      <td>17571333539</td>\n",
              "      <td>completed</td>\n",
              "      <td>success</td>\n",
              "      <td>dynamic</td>\n",
              "      <td>https://github.com/Pony-House/Client/actions/r...</td>\n",
              "      <td>2025-09-09T04:01:05Z</td>\n",
              "      <td>2025-09-09T04:01:56Z</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>undershows/gigs</td>\n",
              "      <td>main</td>\n",
              "      <td>main</td>\n",
              "      <td>d7200937bec082870c02878b1e97a7c637cfe24b</td>\n",
              "      <td>npm_and_yarn in /. for devalue - Update #10972...</td>\n",
              "      <td>17636722869</td>\n",
              "      <td>completed</td>\n",
              "      <td>success</td>\n",
              "      <td>dynamic</td>\n",
              "      <td>https://github.com/undershows/gigs/actions/run...</td>\n",
              "      <td>2025-09-11T07:01:42Z</td>\n",
              "      <td>2025-09-11T07:03:18Z</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>volkswagen/github-app-authentication-action</td>\n",
              "      <td>main</td>\n",
              "      <td>main</td>\n",
              "      <td>7504f5f1674db96adf50d848508cb9d0ff32ef23</td>\n",
              "      <td>npm_and_yarn in /. - Update #983812978</td>\n",
              "      <td>13967443206</td>\n",
              "      <td>in_progress</td>\n",
              "      <td>NaN</td>\n",
              "      <td>dynamic</td>\n",
              "      <td>https://github.com/volkswagen/github-app-authe...</td>\n",
              "      <td>2025-03-20T10:44:43Z</td>\n",
              "      <td>2025-03-20T10:44:51Z</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          repo  branch default_branch  \\\n",
              "0                                 dvgis/dc-sdk  master         master   \n",
              "1         chimoney/chimoney-community-projects    main           main   \n",
              "2                            pony-house/client     dev            dev   \n",
              "3                              undershows/gigs    main           main   \n",
              "4  volkswagen/github-app-authentication-action    main           main   \n",
              "\n",
              "                                     commit  \\\n",
              "0  e810a4a289c04cbd66299fbf53d7e09f5ef7f873   \n",
              "1  e27d3bc8a7211be50c265ed5b8353be85170a39f   \n",
              "2  bcb43f6e0a18f03e40dc058331a2b014ec367b0a   \n",
              "3  d7200937bec082870c02878b1e97a7c637cfe24b   \n",
              "4  7504f5f1674db96adf50d848508cb9d0ff32ef23   \n",
              "\n",
              "                                       workflow_name       run_id  \\\n",
              "0                                              build  17342172601   \n",
              "1                          PR Merge Badge Automation  16342288531   \n",
              "2                  docker in /. - Update #1095030283  17571333539   \n",
              "3  npm_and_yarn in /. for devalue - Update #10972...  17636722869   \n",
              "4             npm_and_yarn in /. - Update #983812978  13967443206   \n",
              "\n",
              "        status conclusion         event  \\\n",
              "0    completed    success          push   \n",
              "1    completed    skipped  pull_request   \n",
              "2    completed    success       dynamic   \n",
              "3    completed    success       dynamic   \n",
              "4  in_progress        NaN       dynamic   \n",
              "\n",
              "                                                 url            start_time  \\\n",
              "0  https://github.com/dvt3d/dc-sdk/actions/runs/1...  2025-08-30T09:32:19Z   \n",
              "1  https://github.com/Chimoney/chimoney-community...  2025-07-17T10:09:03Z   \n",
              "2  https://github.com/Pony-House/Client/actions/r...  2025-09-09T04:01:05Z   \n",
              "3  https://github.com/undershows/gigs/actions/run...  2025-09-11T07:01:42Z   \n",
              "4  https://github.com/volkswagen/github-app-authe...  2025-03-20T10:44:43Z   \n",
              "\n",
              "               end_time  runner_environment  \n",
              "0  2025-08-30T09:34:17Z                 NaN  \n",
              "1  2025-07-17T10:09:04Z                 NaN  \n",
              "2  2025-09-09T04:01:56Z                 NaN  \n",
              "3  2025-09-11T07:03:18Z                 NaN  \n",
              "4  2025-03-20T10:44:51Z                 NaN  "
            ]
          },
          "execution_count": 264,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "BUILD_DATA = \"files/build_data.csv\"\n",
        "\n",
        "df_build_data = pd.read_csv(BUILD_DATA)\n",
        "print(f\"Number of repo with github workflow action: {len(df_build_data)}\")\n",
        "df_build_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "workflow_details_from_ymls = []\n",
        "repos_with_bot_workflows = []\n",
        "\n",
        "def get_workflow_details_from_ymls():\n",
        "    for _, row in df_build_data.iterrows():\n",
        "        repo_name = row['repo'].lower()\n",
        "        path = row['path']\n",
        "        owner = repo_name.split('/')[0].strip()\n",
        "        repo = repo_name.split('/')[1].strip()\n",
        "\n",
        "        if(path.startswith('.github/workflows/') == False):\n",
        "            repos_with_bot_workflows.append(f\"{owner}/{repo}\")\n",
        "            continue\n",
        "        \n",
        "        yml_path = os.path.join(\"workflow_files\", owner, repo, path.split('/')[-1])\n",
        "\n",
        "        with open(yml_path, 'r') as f:\n",
        "\n",
        "            try:\n",
        "                yml = yaml.safe_load(f)\n",
        "\n",
        "            except yaml.YAMLError as e:\n",
        "                print(f\"Error parsing {yml_path}: {e}\")\n",
        "                continue\n",
        "\n",
        "            jobs = yml.get('jobs', {})\n",
        "\n",
        "            jobs_info = []\n",
        "            runs_on_set = set()\n",
        "\n",
        "            for job_id, job in jobs.items():\n",
        "                runs_on = job.get('runs-on', None)\n",
        "                strategy = job.get('strategy', {})\n",
        "                matrix = strategy.get('matrix', {})\n",
        "                steps = job.get('steps', [])\n",
        "                \n",
        "                if isinstance(runs_on, list):\n",
        "                    runs_on_set.update(runs_on)  # use update for lists\n",
        "                else:\n",
        "                    runs_on_set.add(runs_on)\n",
        "                \n",
        "                jobs_info.append({\n",
        "                    \"job_id\": job_id,\n",
        "                    \"runs_on\": runs_on,\n",
        "                    \"matrix\": matrix,\n",
        "                    \"steps\": steps\n",
        "                }  )\n",
        "\n",
        "            workflow_details_from_ymls.append({\n",
        "                \"repo\": repo_name,\n",
        "                \"workflow_name\": yml.get('name', '').strip(),\n",
        "                \"number_of_jobs\": len(jobs),\n",
        "                \"highest_number_of_steps_in_a_job\": max(len(job.get('steps', [])) for job in jobs.values()) if jobs else 0,\n",
        "                \"number_of_steps\": sum(len(job.get('steps', [])) for job in jobs.values()),\n",
        "                \"jobs_info\": jobs_info,\n",
        "                \"operating_systems\": list(runs_on_set),\n",
        "            })\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {},
      "outputs": [],
      "source": [
        "#will fabricated\n",
        "workflow_details_from_ymls = []\n",
        "repos_with_missing_yml = []\n",
        "\n",
        "def get_workflow_details_from_ymls():\n",
        "    for _, row in df_build_data.iterrows():\n",
        "        repo_name = row['repo'].lower()\n",
        "        owner = repo_name.split('/')[0].strip()\n",
        "        repo = repo_name.split('/')[1].strip()\n",
        "\n",
        "        workflow_name = row['workflow_name']\n",
        "\n",
        "        yml_path = os.path.join(\"workflow_files\", owner, repo)\n",
        "        yml_found = False\n",
        "\n",
        "        for root, dirs, files in os.walk(yml_path):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "\n",
        "                if not file.endswith(('.yml', '.yaml')):\n",
        "                    continue\n",
        "                \n",
        "                with open(file_path, 'r') as f:\n",
        "\n",
        "                    try:\n",
        "                        workflow = yaml.safe_load(f)\n",
        "\n",
        "                    except yaml.YAMLError as e:\n",
        "                        print(f\"Error parsing {yml_path}: {e}\")\n",
        "                        continue\n",
        "\n",
        "                    if workflow is None:\n",
        "                        continue\n",
        "\n",
        "                    if not isinstance(workflow, dict):\n",
        "                        continue\n",
        "\n",
        "                    if isinstance(workflow, dict):\n",
        "                        if 'name' not in workflow or workflow.get('name') is None:\n",
        "                            continue\n",
        "            \n",
        "\n",
        "                    if workflow.get('name', '') == workflow_name:\n",
        "                        jobs = workflow.get('jobs', {})\n",
        "\n",
        "                        jobs_info = []\n",
        "                        os_set = set()\n",
        "\n",
        "                        for job_id, job in jobs.items():\n",
        "                            runs_on = job.get('runs-on', None)\n",
        "                            strategy = job.get('strategy', {})\n",
        "                            matrix = strategy.get('matrix', {})\n",
        "                            steps = job.get('steps', [])\n",
        "                            \n",
        "                            if isinstance(runs_on, list):\n",
        "                                os_set.update(runs_on)  # use update for lists\n",
        "                            else:\n",
        "                                os_set.add(runs_on)\n",
        "                            os_set.discard(None)\n",
        "                            \n",
        "                            jobs_info.append({\n",
        "                                \"job_id\": job_id,\n",
        "                                \"runs_on\": runs_on,\n",
        "                                \"matrix\": matrix,\n",
        "                                \"steps\": steps\n",
        "                            })\n",
        "\n",
        "                        workflow_details_from_ymls.append({\n",
        "                            \"repo\": repo_name,\n",
        "                            \"workflow_name\": workflow_name,\n",
        "                            \"number_of_jobs\": len(jobs),\n",
        "                            \"highest_number_of_steps_in_a_job\": max(len(job.get('steps', [])) for job in jobs.values()) if jobs else 0,\n",
        "                            \"jobs_info\": jobs_info,\n",
        "                            \"operating_systems\": list(os_set),\n",
        "                        })\n",
        "                        yml_found = True\n",
        "                        break\n",
        "\n",
        "        if not yml_found:\n",
        "            repos_with_missing_yml.append(f\"{owner}/{repo}\")\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error parsing workflow_files/steve-xmh/amll-ttml-db: while constructing a mapping\n",
            "  in \"workflow_files/steve-xmh/amll-ttml-db/auto-untagged.yml\", line 27, column 17\n",
            "found unhashable key\n",
            "  in \"workflow_files/steve-xmh/amll-ttml-db/auto-untagged.yml\", line 27, column 18\n",
            "Error parsing workflow_files/reactplay/react-play: while parsing a block mapping\n",
            "  in \"workflow_files/reactplay/react-play/playwright-e2e.yml\", line 35, column 11\n",
            "expected <block end>, but found '-'\n",
            "  in \"workflow_files/reactplay/react-play/playwright-e2e.yml\", line 36, column 11\n",
            "Error parsing workflow_files/osc/ondemand: while scanning for the next token\n",
            "found character '\\t' that cannot start any token\n",
            "  in \"workflow_files/osc/ondemand/changelog.yml\", line 24, column 38\n",
            "Error parsing workflow_files/osc/ondemand: while scanning for the next token\n",
            "found character '\\t' that cannot start any token\n",
            "  in \"workflow_files/osc/ondemand/lint.yml\", line 24, column 38\n",
            "Error parsing workflow_files/osc/ondemand: while scanning for the next token\n",
            "found character '\\t' that cannot start any token\n",
            "  in \"workflow_files/osc/ondemand/tests.yml\", line 41, column 38\n",
            "Error parsing workflow_files/thangarajtk/wdio_mocha_javascript: mapping values are not allowed here\n",
            "  in \"workflow_files/thangarajtk/wdio_mocha_javascript/node.js.yml\", line 35, column 13\n",
            "Error parsing workflow_files/montumodi/sqs-bulk-loader: while scanning for the next token\n",
            "found character '\\t' that cannot start any token\n",
            "  in \"workflow_files/montumodi/sqs-bulk-loader/npm-publish.yml\", line 42, column 13\n",
            "Error parsing workflow_files/montumodi/mongodb-atlas-api-client: while scanning for the next token\n",
            "found character '\\t' that cannot start any token\n",
            "  in \"workflow_files/montumodi/mongodb-atlas-api-client/npmpublish.yml\", line 43, column 13\n",
            "Error parsing workflow_files/colyseus/colyseus-construct3: unacceptable character #x0008: special characters are not allowed\n",
            "  in \"workflow_files/colyseus/colyseus-construct3/upload-release.yml\", position 1454\n",
            "Error parsing workflow_files/pradumnasaraf/devops: while scanning for the next token\n",
            "found character '\\t' that cannot start any token\n",
            "  in \"workflow_files/pradumnasaraf/devops/releases.yml\", line 28, column 14\n",
            "Error parsing workflow_files/pradumnasaraf/open-source-with-pradumna: while scanning for the next token\n",
            "found character '\\t' that cannot start any token\n",
            "  in \"workflow_files/pradumnasaraf/open-source-with-pradumna/releases.yml\", line 28, column 14\n",
            "Error parsing workflow_files/tiledesk/tiledesk-chatbot: while scanning for the next token\n",
            "found character '\\t' that cannot start any token\n",
            "  in \"workflow_files/tiledesk/tiledesk-chatbot/docker-community-push-latest.yaml\", line 7, column 4\n",
            "Error parsing workflow_files/tiledesk/tiledesk-chatbot: while scanning for the next token\n",
            "found character '\\t' that cannot start any token\n",
            "  in \"workflow_files/tiledesk/tiledesk-chatbot/docker-image-tag-community-tag-push.yaml\", line 24, column 12\n",
            "Error parsing workflow_files/tiledesk/tiledesk-server: while scanning for the next token\n",
            "found character '\\t' that cannot start any token\n",
            "  in \"workflow_files/tiledesk/tiledesk-server/docker-push-en-push-latest.yml\", line 3, column 4\n",
            "Error parsing workflow_files/tiledesk/tiledesk-server: while scanning for the next token\n",
            "found character '\\t' that cannot start any token\n",
            "  in \"workflow_files/tiledesk/tiledesk-server/docker-community-profiler-latest.yml\", line 3, column 4\n",
            "Error parsing workflow_files/tiledesk/tiledesk-server: while scanning for the next token\n",
            "found character '\\t' that cannot start any token\n",
            "  in \"workflow_files/tiledesk/tiledesk-server/docker-image-en-tag-push.yml\", line 18, column 13\n",
            "Error parsing workflow_files/tiledesk/tiledesk-server: while scanning for the next token\n",
            "found character '\\t' that cannot start any token\n",
            "  in \"workflow_files/tiledesk/tiledesk-server/docker-community-worker-push-latest.yml\", line 3, column 4\n",
            "Error parsing workflow_files/tiledesk/tiledesk-server: while scanning for the next token\n",
            "found character '\\t' that cannot start any token\n",
            "  in \"workflow_files/tiledesk/tiledesk-server/docker-community-push-latest.yml\", line 3, column 4\n",
            "Error parsing workflow_files/zhangbo126/threejs-3dmodel-edit: expected '<document start>', but found '<block mapping start>'\n",
            "  in \"workflow_files/zhangbo126/threejs-3dmodel-edit/jekyll-gh-pages.yml\", line 2, column 3\n",
            "Error parsing workflow_files/meirim-org/meirim: while scanning for the next token\n",
            "found character '\\t' that cannot start any token\n",
            "  in \"workflow_files/meirim-org/meirim/server.yml\", line 4, column 27\n",
            "Error parsing workflow_files/adobe/commerce-cif-graphql-integration-reference: mapping values are not allowed here\n",
            "  in \"workflow_files/adobe/commerce-cif-graphql-integration-reference/deploy_stage.yml\", line 46, column 122\n",
            "Error parsing workflow_files/foxthefox/iobroker.musiccast: while parsing a block collection\n",
            "  in \"workflow_files/foxthefox/iobroker.musiccast/dependabot-auto-merge.yml\", line 13, column 5\n",
            "expected <block end>, but found '?'\n",
            "  in \"workflow_files/foxthefox/iobroker.musiccast/dependabot-auto-merge.yml\", line 14, column 5\n",
            "Error parsing workflow_files/foxthefox/yamaha-yxc-nodejs: while scanning for the next token\n",
            "found character '\\t' that cannot start any token\n",
            "  in \"workflow_files/foxthefox/yamaha-yxc-nodejs/dependabot-auto-merge.yml\", line 11, column 1\n",
            "Error parsing workflow_files/vanshking30/foodiesweb: mapping values are not allowed here\n",
            "  in \"workflow_files/vanshking30/foodiesweb/close-old-pr.yml\", line 2, column 5\n",
            "Error parsing workflow_files/vanshking30/foodiesweb: mapping values are not allowed here\n",
            "  in \"workflow_files/vanshking30/foodiesweb/autocomment-iss-close.yml\", line 2, column 5\n",
            "Error parsing workflow_files/diegomura/react-pdf: mapping values are not allowed here\n",
            "  in \"workflow_files/diegomura/react-pdf/main.yml\", line 114, column 101\n",
            "Error parsing workflow_files/lucaszhu2zgf/mp-progress: mapping values are not allowed here\n",
            "  in \"workflow_files/lucaszhu2zgf/mp-progress/nodejs.yml\", line 25, column 17\n",
            "Error parsing workflow_files/jgibbons-cp/datadog: while scanning a simple key\n",
            "  in \"workflow_files/jgibbons-cp/datadog/win_docker_build.yaml\", line 8, column 1\n",
            "could not find expected ':'\n",
            "  in \"workflow_files/jgibbons-cp/datadog/win_docker_build.yaml\", line 9, column 16\n",
            "Error parsing workflow_files/bbaudry/swart-studio: while scanning for the next token\n",
            "found character '\\t' that cannot start any token\n",
            "  in \"workflow_files/bbaudry/swart-studio/art.yml\", line 10, column 95\n",
            "Error parsing workflow_files/somethingnew2-0/simplecspm: while scanning for the next token\n",
            "found character '\\t' that cannot start any token\n",
            "  in \"workflow_files/somethingnew2-0/simplecspm/deploy.yml\", line 51, column 33\n"
          ]
        }
      ],
      "source": [
        "get_workflow_details_from_ymls()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of workflow details from ymls: 8206\n"
          ]
        }
      ],
      "source": [
        "# workflow_details_from_ymls to dataframe\n",
        "df_workflow_details_from_ymls = pd.DataFrame(workflow_details_from_ymls)\n",
        "print(f\"Number of workflow details from ymls: {len(df_workflow_details_from_ymls)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>repo</th>\n",
              "      <th>workflow_name</th>\n",
              "      <th>number_of_jobs</th>\n",
              "      <th>highest_number_of_steps_in_a_job</th>\n",
              "      <th>jobs_info</th>\n",
              "      <th>operating_systems</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>dvgis/dc-sdk</td>\n",
              "      <td>build</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>[{'job_id': 'build', 'runs_on': 'ubuntu-latest...</td>\n",
              "      <td>[ubuntu-latest]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>chimoney/chimoney-community-projects</td>\n",
              "      <td>PR Merge Badge Automation</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>[{'job_id': 'badge_automation', 'runs_on': 'ub...</td>\n",
              "      <td>[ubuntu-latest]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>mateonunez/website</td>\n",
              "      <td>ci</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>[{'job_id': 'ci', 'runs_on': '${{ matrix.os }}...</td>\n",
              "      <td>[${{ matrix.os }}]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>svg/svgo</td>\n",
              "      <td>CodeQL</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>[{'job_id': 'analyze', 'runs_on': 'ubuntu-late...</td>\n",
              "      <td>[ubuntu-latest]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>lupyuen/nuttx-ox64</td>\n",
              "      <td>Daily Test of NuttX for Ox64</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>[{'job_id': 'test', 'runs_on': 'ubuntu-latest'...</td>\n",
              "      <td>[ubuntu-latest]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   repo                 workflow_name  \\\n",
              "0                          dvgis/dc-sdk                         build   \n",
              "1  chimoney/chimoney-community-projects     PR Merge Badge Automation   \n",
              "2                    mateonunez/website                            ci   \n",
              "3                              svg/svgo                        CodeQL   \n",
              "4                    lupyuen/nuttx-ox64  Daily Test of NuttX for Ox64   \n",
              "\n",
              "   number_of_jobs  highest_number_of_steps_in_a_job  \\\n",
              "0               1                                 3   \n",
              "1               1                                 4   \n",
              "2               1                                 6   \n",
              "3               1                                 4   \n",
              "4               1                                 6   \n",
              "\n",
              "                                           jobs_info   operating_systems  \n",
              "0  [{'job_id': 'build', 'runs_on': 'ubuntu-latest...     [ubuntu-latest]  \n",
              "1  [{'job_id': 'badge_automation', 'runs_on': 'ub...     [ubuntu-latest]  \n",
              "2  [{'job_id': 'ci', 'runs_on': '${{ matrix.os }}...  [${{ matrix.os }}]  \n",
              "3  [{'job_id': 'analyze', 'runs_on': 'ubuntu-late...     [ubuntu-latest]  \n",
              "4  [{'job_id': 'test', 'runs_on': 'ubuntu-latest'...     [ubuntu-latest]  "
            ]
          },
          "execution_count": 255,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# header of df_workflow_details_from_ymls\n",
        "df_workflow_details_from_ymls.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>8203</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>repo</th>\n",
              "      <td>quarto-dev/quarto-web</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>workflow_name</th>\n",
              "      <td>Update Downloads</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>number_of_jobs</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>highest_number_of_steps_in_a_job</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>jobs_info</th>\n",
              "      <td>[{'job_id': 'update-downloads', 'runs_on': 'ub...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>operating_systems</th>\n",
              "      <td>[None, ubuntu-latest]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                               8203\n",
              "repo                                                          quarto-dev/quarto-web\n",
              "workflow_name                                                      Update Downloads\n",
              "number_of_jobs                                                                    3\n",
              "highest_number_of_steps_in_a_job                                                  4\n",
              "jobs_info                         [{'job_id': 'update-downloads', 'runs_on': 'ub...\n",
              "operating_systems                                             [None, ubuntu-latest]"
            ]
          },
          "execution_count": 248,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# df_workflow_details_from_ymls full details for repo 'quarto-dev/quarto-web'\n",
        "df_workflow_details_from_ymls[df_workflow_details_from_ymls['repo'] == 'quarto-dev/quarto-web'].T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 257,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>repo</th>\n",
              "      <th>branch</th>\n",
              "      <th>default_branch</th>\n",
              "      <th>commit</th>\n",
              "      <th>workflow_name_ci</th>\n",
              "      <th>run_id</th>\n",
              "      <th>status</th>\n",
              "      <th>conclusion</th>\n",
              "      <th>event</th>\n",
              "      <th>url</th>\n",
              "      <th>start_time</th>\n",
              "      <th>end_time</th>\n",
              "      <th>runner_environment</th>\n",
              "      <th>workflow_name_yml</th>\n",
              "      <th>number_of_jobs</th>\n",
              "      <th>highest_number_of_steps_in_a_job</th>\n",
              "      <th>jobs_info</th>\n",
              "      <th>operating_systems</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>dvgis/dc-sdk</td>\n",
              "      <td>master</td>\n",
              "      <td>master</td>\n",
              "      <td>e810a4a289c04cbd66299fbf53d7e09f5ef7f873</td>\n",
              "      <td>build</td>\n",
              "      <td>17342172601</td>\n",
              "      <td>completed</td>\n",
              "      <td>success</td>\n",
              "      <td>push</td>\n",
              "      <td>https://github.com/dvt3d/dc-sdk/actions/runs/1...</td>\n",
              "      <td>2025-08-30T09:32:19Z</td>\n",
              "      <td>2025-08-30T09:34:17Z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>build</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>[{'job_id': 'build', 'runs_on': 'ubuntu-latest...</td>\n",
              "      <td>[ubuntu-latest]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>chimoney/chimoney-community-projects</td>\n",
              "      <td>main</td>\n",
              "      <td>main</td>\n",
              "      <td>e27d3bc8a7211be50c265ed5b8353be85170a39f</td>\n",
              "      <td>PR Merge Badge Automation</td>\n",
              "      <td>16342288531</td>\n",
              "      <td>completed</td>\n",
              "      <td>skipped</td>\n",
              "      <td>pull_request</td>\n",
              "      <td>https://github.com/Chimoney/chimoney-community...</td>\n",
              "      <td>2025-07-17T10:09:03Z</td>\n",
              "      <td>2025-07-17T10:09:04Z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>PR Merge Badge Automation</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>[{'job_id': 'badge_automation', 'runs_on': 'ub...</td>\n",
              "      <td>[ubuntu-latest]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>pony-house/client</td>\n",
              "      <td>dev</td>\n",
              "      <td>dev</td>\n",
              "      <td>bcb43f6e0a18f03e40dc058331a2b014ec367b0a</td>\n",
              "      <td>docker in /. - Update #1095030283</td>\n",
              "      <td>17571333539</td>\n",
              "      <td>completed</td>\n",
              "      <td>success</td>\n",
              "      <td>dynamic</td>\n",
              "      <td>https://github.com/Pony-House/Client/actions/r...</td>\n",
              "      <td>2025-09-09T04:01:05Z</td>\n",
              "      <td>2025-09-09T04:01:56Z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>undershows/gigs</td>\n",
              "      <td>main</td>\n",
              "      <td>main</td>\n",
              "      <td>d7200937bec082870c02878b1e97a7c637cfe24b</td>\n",
              "      <td>npm_and_yarn in /. for devalue - Update #10972...</td>\n",
              "      <td>17636722869</td>\n",
              "      <td>completed</td>\n",
              "      <td>success</td>\n",
              "      <td>dynamic</td>\n",
              "      <td>https://github.com/undershows/gigs/actions/run...</td>\n",
              "      <td>2025-09-11T07:01:42Z</td>\n",
              "      <td>2025-09-11T07:03:18Z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>volkswagen/github-app-authentication-action</td>\n",
              "      <td>main</td>\n",
              "      <td>main</td>\n",
              "      <td>7504f5f1674db96adf50d848508cb9d0ff32ef23</td>\n",
              "      <td>npm_and_yarn in /. - Update #983812978</td>\n",
              "      <td>13967443206</td>\n",
              "      <td>in_progress</td>\n",
              "      <td>NaN</td>\n",
              "      <td>dynamic</td>\n",
              "      <td>https://github.com/volkswagen/github-app-authe...</td>\n",
              "      <td>2025-03-20T10:44:43Z</td>\n",
              "      <td>2025-03-20T10:44:51Z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11070</th>\n",
              "      <td>jsebrech/plainvanilla</td>\n",
              "      <td>main</td>\n",
              "      <td>main</td>\n",
              "      <td>745034afa11be22cf43ea97d2ddc54b1e1f5758f</td>\n",
              "      <td>Deploy static content to Pages</td>\n",
              "      <td>16693957140</td>\n",
              "      <td>completed</td>\n",
              "      <td>success</td>\n",
              "      <td>push</td>\n",
              "      <td>https://github.com/jsebrech/plainvanilla/actio...</td>\n",
              "      <td>2025-08-02T13:11:46Z</td>\n",
              "      <td>2025-08-02T13:12:04Z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Deploy static content to Pages</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>[{'job_id': 'deploy', 'runs_on': 'ubuntu-lates...</td>\n",
              "      <td>[ubuntu-latest]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11071</th>\n",
              "      <td>quarto-dev/quarto-cli</td>\n",
              "      <td>main</td>\n",
              "      <td>main</td>\n",
              "      <td>1ca3dabc13786e3bf91b70eb3333185c799b3414</td>\n",
              "      <td>Performance Check</td>\n",
              "      <td>17654586346</td>\n",
              "      <td>completed</td>\n",
              "      <td>success</td>\n",
              "      <td>schedule</td>\n",
              "      <td>https://github.com/quarto-dev/quarto-cli/actio...</td>\n",
              "      <td>2025-09-11T19:03:00Z</td>\n",
              "      <td>2025-09-11T19:04:29Z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Performance Check</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>[{'job_id': 'test-bundle', 'runs_on': 'ubuntu-...</td>\n",
              "      <td>[ubuntu-latest]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11072</th>\n",
              "      <td>quarto-dev/quarto-web</td>\n",
              "      <td>main</td>\n",
              "      <td>main</td>\n",
              "      <td>88fff8bb9f3827f0b1c577335424c8482eb554c3</td>\n",
              "      <td>Update Downloads</td>\n",
              "      <td>17655026497</td>\n",
              "      <td>completed</td>\n",
              "      <td>success</td>\n",
              "      <td>schedule</td>\n",
              "      <td>https://github.com/quarto-dev/quarto-web/actio...</td>\n",
              "      <td>2025-09-11T19:21:10Z</td>\n",
              "      <td>2025-09-11T19:21:43Z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Update Downloads</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>[{'job_id': 'update-downloads', 'runs_on': 'ub...</td>\n",
              "      <td>[ubuntu-latest]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11073</th>\n",
              "      <td>newan/iobroker.bluelink</td>\n",
              "      <td>master</td>\n",
              "      <td>master</td>\n",
              "      <td>40ac86540cbf5ac723eb7b96cc56d3cd0e7320aa</td>\n",
              "      <td>CodeQL</td>\n",
              "      <td>17530721044</td>\n",
              "      <td>completed</td>\n",
              "      <td>success</td>\n",
              "      <td>schedule</td>\n",
              "      <td>https://github.com/Newan/ioBroker.bluelink/act...</td>\n",
              "      <td>2025-09-07T15:56:06Z</td>\n",
              "      <td>2025-09-07T15:57:21Z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>CodeQL</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>[{'job_id': 'analyze', 'runs_on': 'ubuntu-late...</td>\n",
              "      <td>[ubuntu-latest]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11074</th>\n",
              "      <td>spcfox/amnezia-wg-easy</td>\n",
              "      <td>master</td>\n",
              "      <td>master</td>\n",
              "      <td>235caf179fb66c2c175de0f0eae110089898ec4b</td>\n",
              "      <td>CodeQL</td>\n",
              "      <td>17631738303</td>\n",
              "      <td>completed</td>\n",
              "      <td>success</td>\n",
              "      <td>schedule</td>\n",
              "      <td>https://github.com/spcfox/amnezia-wg-easy/acti...</td>\n",
              "      <td>2025-09-11T02:00:05Z</td>\n",
              "      <td>2025-09-11T02:01:16Z</td>\n",
              "      <td>NaN</td>\n",
              "      <td>CodeQL</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>[{'job_id': 'analyze', 'runs_on': 'ubuntu-late...</td>\n",
              "      <td>[ubuntu-latest]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11075 rows  18 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              repo  branch default_branch  \\\n",
              "0                                     dvgis/dc-sdk  master         master   \n",
              "1             chimoney/chimoney-community-projects    main           main   \n",
              "2                                pony-house/client     dev            dev   \n",
              "3                                  undershows/gigs    main           main   \n",
              "4      volkswagen/github-app-authentication-action    main           main   \n",
              "...                                            ...     ...            ...   \n",
              "11070                        jsebrech/plainvanilla    main           main   \n",
              "11071                        quarto-dev/quarto-cli    main           main   \n",
              "11072                        quarto-dev/quarto-web    main           main   \n",
              "11073                      newan/iobroker.bluelink  master         master   \n",
              "11074                       spcfox/amnezia-wg-easy  master         master   \n",
              "\n",
              "                                         commit  \\\n",
              "0      e810a4a289c04cbd66299fbf53d7e09f5ef7f873   \n",
              "1      e27d3bc8a7211be50c265ed5b8353be85170a39f   \n",
              "2      bcb43f6e0a18f03e40dc058331a2b014ec367b0a   \n",
              "3      d7200937bec082870c02878b1e97a7c637cfe24b   \n",
              "4      7504f5f1674db96adf50d848508cb9d0ff32ef23   \n",
              "...                                         ...   \n",
              "11070  745034afa11be22cf43ea97d2ddc54b1e1f5758f   \n",
              "11071  1ca3dabc13786e3bf91b70eb3333185c799b3414   \n",
              "11072  88fff8bb9f3827f0b1c577335424c8482eb554c3   \n",
              "11073  40ac86540cbf5ac723eb7b96cc56d3cd0e7320aa   \n",
              "11074  235caf179fb66c2c175de0f0eae110089898ec4b   \n",
              "\n",
              "                                        workflow_name_ci       run_id  \\\n",
              "0                                                  build  17342172601   \n",
              "1                              PR Merge Badge Automation  16342288531   \n",
              "2                      docker in /. - Update #1095030283  17571333539   \n",
              "3      npm_and_yarn in /. for devalue - Update #10972...  17636722869   \n",
              "4                 npm_and_yarn in /. - Update #983812978  13967443206   \n",
              "...                                                  ...          ...   \n",
              "11070                     Deploy static content to Pages  16693957140   \n",
              "11071                                  Performance Check  17654586346   \n",
              "11072                                   Update Downloads  17655026497   \n",
              "11073                                             CodeQL  17530721044   \n",
              "11074                                             CodeQL  17631738303   \n",
              "\n",
              "            status conclusion         event  \\\n",
              "0        completed    success          push   \n",
              "1        completed    skipped  pull_request   \n",
              "2        completed    success       dynamic   \n",
              "3        completed    success       dynamic   \n",
              "4      in_progress        NaN       dynamic   \n",
              "...            ...        ...           ...   \n",
              "11070    completed    success          push   \n",
              "11071    completed    success      schedule   \n",
              "11072    completed    success      schedule   \n",
              "11073    completed    success      schedule   \n",
              "11074    completed    success      schedule   \n",
              "\n",
              "                                                     url  \\\n",
              "0      https://github.com/dvt3d/dc-sdk/actions/runs/1...   \n",
              "1      https://github.com/Chimoney/chimoney-community...   \n",
              "2      https://github.com/Pony-House/Client/actions/r...   \n",
              "3      https://github.com/undershows/gigs/actions/run...   \n",
              "4      https://github.com/volkswagen/github-app-authe...   \n",
              "...                                                  ...   \n",
              "11070  https://github.com/jsebrech/plainvanilla/actio...   \n",
              "11071  https://github.com/quarto-dev/quarto-cli/actio...   \n",
              "11072  https://github.com/quarto-dev/quarto-web/actio...   \n",
              "11073  https://github.com/Newan/ioBroker.bluelink/act...   \n",
              "11074  https://github.com/spcfox/amnezia-wg-easy/acti...   \n",
              "\n",
              "                 start_time              end_time  runner_environment  \\\n",
              "0      2025-08-30T09:32:19Z  2025-08-30T09:34:17Z                 NaN   \n",
              "1      2025-07-17T10:09:03Z  2025-07-17T10:09:04Z                 NaN   \n",
              "2      2025-09-09T04:01:05Z  2025-09-09T04:01:56Z                 NaN   \n",
              "3      2025-09-11T07:01:42Z  2025-09-11T07:03:18Z                 NaN   \n",
              "4      2025-03-20T10:44:43Z  2025-03-20T10:44:51Z                 NaN   \n",
              "...                     ...                   ...                 ...   \n",
              "11070  2025-08-02T13:11:46Z  2025-08-02T13:12:04Z                 NaN   \n",
              "11071  2025-09-11T19:03:00Z  2025-09-11T19:04:29Z                 NaN   \n",
              "11072  2025-09-11T19:21:10Z  2025-09-11T19:21:43Z                 NaN   \n",
              "11073  2025-09-07T15:56:06Z  2025-09-07T15:57:21Z                 NaN   \n",
              "11074  2025-09-11T02:00:05Z  2025-09-11T02:01:16Z                 NaN   \n",
              "\n",
              "                    workflow_name_yml  number_of_jobs  \\\n",
              "0                               build             1.0   \n",
              "1           PR Merge Badge Automation             1.0   \n",
              "2                                 NaN             NaN   \n",
              "3                                 NaN             NaN   \n",
              "4                                 NaN             NaN   \n",
              "...                               ...             ...   \n",
              "11070  Deploy static content to Pages             1.0   \n",
              "11071               Performance Check             1.0   \n",
              "11072                Update Downloads             3.0   \n",
              "11073                          CodeQL             1.0   \n",
              "11074                          CodeQL             1.0   \n",
              "\n",
              "       highest_number_of_steps_in_a_job  \\\n",
              "0                                   3.0   \n",
              "1                                   4.0   \n",
              "2                                   NaN   \n",
              "3                                   NaN   \n",
              "4                                   NaN   \n",
              "...                                 ...   \n",
              "11070                               5.0   \n",
              "11071                               7.0   \n",
              "11072                               4.0   \n",
              "11073                               4.0   \n",
              "11074                               4.0   \n",
              "\n",
              "                                               jobs_info operating_systems  \n",
              "0      [{'job_id': 'build', 'runs_on': 'ubuntu-latest...   [ubuntu-latest]  \n",
              "1      [{'job_id': 'badge_automation', 'runs_on': 'ub...   [ubuntu-latest]  \n",
              "2                                                    NaN               NaN  \n",
              "3                                                    NaN               NaN  \n",
              "4                                                    NaN               NaN  \n",
              "...                                                  ...               ...  \n",
              "11070  [{'job_id': 'deploy', 'runs_on': 'ubuntu-lates...   [ubuntu-latest]  \n",
              "11071  [{'job_id': 'test-bundle', 'runs_on': 'ubuntu-...   [ubuntu-latest]  \n",
              "11072  [{'job_id': 'update-downloads', 'runs_on': 'ub...   [ubuntu-latest]  \n",
              "11073  [{'job_id': 'analyze', 'runs_on': 'ubuntu-late...   [ubuntu-latest]  \n",
              "11074  [{'job_id': 'analyze', 'runs_on': 'ubuntu-late...   [ubuntu-latest]  \n",
              "\n",
              "[11075 rows x 18 columns]"
            ]
          },
          "execution_count": 257,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# merge df_ci_data and df_workflow_details_from_ymls on 'repo'\n",
        "df_merged = pd.merge(df_build_data, df_workflow_details_from_ymls, on='repo', how='left', suffixes=('_ci', '_yml'))\n",
        "df_merged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 266,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_merged.to_csv(\"files/merged_ci_workflow_details.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# iterate items in operating_systems column and remove entries that contain special characters like $ { { } }\n",
        "df_merged['operating_systems'] = df_merged['operating_systems'].apply(lambda os_list: [os for os in os_list if all(c.isalnum() or c in ('-', '_', '.') for c in os)] if isinstance(os_list, list) else os_list)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
